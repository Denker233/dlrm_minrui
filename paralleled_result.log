===============================================
===============================================

=== RUNNING dlrm_s_pytorch.py ===
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading raw data=./input/train.txt
Reading data from path=./input/train.txt
Load 6549/654866 (1%) Split: 0  Label True: 1  Stored: 1
Load 13098/654866 (2%) Split: 0  Label True: 0  Stored: 0
Load 19646/654866 (3%) Split: 0  Label True: 0  Stored: 0
Load 26195/654866 (4%) Split: 0  Label True: 0  Stored: 0
Load 32744/654866 (5%) Split: 0  Label True: 0  Stored: 0
Load 39292/654866 (6%) Split: 0  Label True: 0  Stored: 0
Load 45841/654866 (7%) Split: 0  Label True: 0  Stored: 0
Load 52390/654866 (8%) Split: 0  Label True: 0  Stored: 0
Load 58938/654866 (9%) Split: 0  Label True: 0  Stored: 0
Load 65487/654866 (10%) Split: 0  Label True: 1  Stored: 1
Load 72036/654866 (11%) Split: 0  Label True: 0  Stored: 0
Load 78584/654866 (12%) Split: 0  Label True: 0  Stored: 0
Load 85133/654866 (13%) Split: 0  Label True: 1  Stored: 1
Load 91682/654866 (14%) Split: 0  Label True: 0  Stored: 0
Load 98230/654866 (15%) Split: 0  Label True: 0  Stored: 0
Load 104779/654866 (16%) Split: 0  Label True: 0  Stored: 0
Load 111328/654866 (17%) Split: 0  Label True: 0  Stored: 0
Load 117876/654866 (18%) Split: 0  Label True: 0  Stored: 0
Load 124425/654866 (19%) Split: 0  Label True: 0  Stored: 0
Load 130974/654866 (20%) Split: 0  Label True: 0  Stored: 0
Load 137522/654866 (21%) Split: 0  Label True: 0  Stored: 0
Load 144071/654866 (22%) Split: 0  Label True: 0  Stored: 0
Load 150620/654866 (23%) Split: 0  Label True: 0  Stored: 0
Load 157168/654866 (24%) Split: 0  Label True: 1  Stored: 1
Load 163717/654866 (25%) Split: 0  Label True: 1  Stored: 1
Load 170266/654866 (26%) Split: 0  Label True: 0  Stored: 0
Load 176814/654866 (27%) Split: 0  Label True: 0  Stored: 0
Load 183363/654866 (28%) Split: 0  Label True: 0  Stored: 0
Load 189912/654866 (29%) Split: 0  Label True: 0  Stored: 0
Load 196460/654866 (30%) Split: 0  Label True: 0  Stored: 0
Load 203009/654866 (31%) Split: 0  Label True: 0  Stored: 0
Load 209558/654866 (32%) Split: 0  Label True: 0  Stored: 0
Load 216106/654866 (33%) Split: 0  Label True: 0  Stored: 0
Load 222655/654866 (34%) Split: 0  Label True: 1  Stored: 1
Load 229204/654866 (35%) Split: 0  Label True: 1  Stored: 1
Load 235752/654866 (36%) Split: 0  Label True: 1  Stored: 1
Load 242301/654866 (37%) Split: 0  Label True: 0  Stored: 0
Load 248850/654866 (38%) Split: 0  Label True: 0  Stored: 0
Load 255398/654866 (39%) Split: 0  Label True: 0  Stored: 0
Load 261947/654866 (40%) Split: 0  Label True: 0  Stored: 0
Load 268496/654866 (41%) Split: 0  Label True: 0  Stored: 0
Load 275044/654866 (42%) Split: 0  Label True: 0  Stored: 0
Load 281593/654866 (43%) Split: 0  Label True: 0  Stored: 0
Load 288142/654866 (44%) Split: 0  Label True: 0  Stored: 0
Load 294690/654866 (45%) Split: 0  Label True: 0  Stored: 0
Load 301239/654866 (46%) Split: 0  Label True: 0  Stored: 0
Load 307788/654866 (47%) Split: 0  Label True: 0  Stored: 0
Load 314336/654866 (48%) Split: 0  Label True: 1  Stored: 1
Load 320885/654866 (49%) Split: 0  Label True: 1  Stored: 1
Load 327434/654866 (50%) Split: 0  Label True: 0  Stored: 0
Load 333982/654866 (51%) Split: 0  Label True: 0  Stored: 0
Load 340531/654866 (52%) Split: 0  Label True: 1  Stored: 1
Load 347079/654866 (53%) Split: 0  Label True: 0  Stored: 0
Load 353628/654866 (54%) Split: 0  Label True: 0  Stored: 0
Load 360177/654866 (55%) Split: 0  Label True: 1  Stored: 1
Load 366725/654866 (56%) Split: 0  Label True: 0  Stored: 0
Load 373274/654866 (57%) Split: 0  Label True: 1  Stored: 1
Load 379823/654866 (58%) Split: 0  Label True: 0  Stored: 0
Load 386371/654866 (59%) Split: 0  Label True: 0  Stored: 0
Load 392920/654866 (60%) Split: 0  Label True: 1  Stored: 1
Load 399469/654866 (61%) Split: 0  Label True: 0  Stored: 0
Load 406017/654866 (62%) Split: 0  Label True: 0  Stored: 0
Load 412566/654866 (63%) Split: 0  Label True: 1  Stored: 1
Load 419115/654866 (64%) Split: 0  Label True: 0  Stored: 0
Load 425663/654866 (65%) Split: 0  Label True: 0  Stored: 0
Load 432212/654866 (66%) Split: 0  Label True: 0  Stored: 0
Load 438761/654866 (67%) Split: 0  Label True: 1  Stored: 1
Load 445309/654866 (68%) Split: 0  Label True: 1  Stored: 1
Load 451858/654866 (69%) Split: 0  Label True: 0  Stored: 0
Load 458407/654866 (70%) Split: 0  Label True: 0  Stored: 0
Load 464955/654866 (71%) Split: 0  Label True: 0  Stored: 0
Load 471504/654866 (72%) Split: 0  Label True: 0  Stored: 0
Load 478053/654866 (73%) Split: 0  Label True: 0  Stored: 0
Load 484601/654866 (74%) Split: 0  Label True: 0  Stored: 0
Load 491150/654866 (75%) Split: 0  Label True: 0  Stored: 0
Load 497699/654866 (76%) Split: 0  Label True: 0  Stored: 0
Load 504247/654866 (77%) Split: 0  Label True: 0  Stored: 0
Load 510796/654866 (78%) Split: 0  Label True: 0  Stored: 0
Load 517345/654866 (79%) Split: 0  Label True: 1  Stored: 1
Load 523893/654866 (80%) Split: 0  Label True: 0  Stored: 0
Load 530442/654866 (81%) Split: 0  Label True: 0  Stored: 0
Load 536991/654866 (82%) Split: 0  Label True: 0  Stored: 0
Load 543539/654866 (83%) Split: 0  Label True: 0  Stored: 0
Load 550088/654866 (84%) Split: 0  Label True: 1  Stored: 1
Load 556637/654866 (85%) Split: 0  Label True: 0  Stored: 0
Load 563185/654866 (86%) Split: 0  Label True: 0  Stored: 0
Load 569734/654866 (87%) Split: 0  Label True: 0  Stored: 0
Load 576283/654866 (88%) Split: 0  Label True: 1  Stored: 1
Load 582831/654866 (89%) Split: 0  Label True: 0  Stored: 0
Load 589380/654866 (90%) Split: 0  Label True: 0  Stored: 0
Load 595929/654866 (91%) Split: 0  Label True: 0  Stored: 0
Load 602477/654866 (92%) Split: 0  Label True: 0  Stored: 0
Load 609026/654866 (93%) Split: 0  Label True: 1  Stored: 1
Load 615575/654866 (94%) Split: 0  Label True: 0  Stored: 0
Load 622123/654866 (95%) Split: 0  Label True: 0  Stored: 0
Load 628672/654866 (96%) Split: 0  Label True: 0  Stored: 0
Load 635221/654866 (97%) Split: 0  Label True: 0  Stored: 0
Load 641769/654866 (98%) Split: 0  Label True: 0  Stored: 0
Load 648318/654866 (99%) Split: 0  Label True: 0  Stored: 0

Saved ./input/train_day_0.npz!
Load 6549/654866 (1%) Split: 4  Label True: 1  Stored: 1
Load 13098/654866 (2%) Split: 4  Label True: 1  Stored: 1
Load 19646/654866 (3%) Split: 4  Label True: 1  Stored: 1
Load 26195/654866 (4%) Split: 4  Label True: 0  Stored: 0
Load 32744/654866 (5%) Split: 4  Label True: 1  Stored: 1
Load 39292/654866 (6%) Split: 4  Label True: 0  Stored: 0
Load 45841/654866 (7%) Split: 4  Label True: 0  Stored: 0
Load 52390/654866 (8%) Split: 4  Label True: 0  Stored: 0
Load 58938/654866 (9%) Split: 4  Label True: 0  Stored: 0
Load 65487/654866 (10%) Split: 4  Label True: 0  Stored: 0
Load 72036/654866 (11%) Split: 4  Label True: 0  Stored: 0
Load 78584/654866 (12%) Split: 4  Label True: 0  Stored: 0
Load 85133/654866 (13%) Split: 4  Label True: 1  Stored: 1
Load 91682/654866 (14%) Split: 4  Label True: 1  Stored: 1
Load 98230/654866 (15%) Split: 4  Label True: 0  Stored: 0
Load 104779/654866 (16%) Split: 4  Label True: 1  Stored: 1
Load 111328/654866 (17%) Split: 4  Label True: 0  Stored: 0
Load 117876/654866 (18%) Split: 4  Label True: 0  Stored: 0
Load 124425/654866 (19%) Split: 4  Label True: 1  Stored: 1
Load 130974/654866 (20%) Split: 4  Label True: 0  Stored: 0
Load 137522/654866 (21%) Split: 4  Label True: 0  Stored: 0
Load 144071/654866 (22%) Split: 4  Label True: 0  Stored: 0
Load 150620/654866 (23%) Split: 4  Label True: 0  Stored: 0
Load 157168/654866 (24%) Split: 4  Label True: 0  Stored: 0
Load 163717/654866 (25%) Split: 4  Label True: 0  Stored: 0
Load 170266/654866 (26%) Split: 4  Label True: 0  Stored: 0
Load 176814/654866 (27%) Split: 4  Label True: 0  Stored: 0
Load 183363/654866 (28%) Split: 4  Label True: 0  Stored: 0
Load 189912/654866 (29%) Split: 4  Label True: 1  Stored: 1
Load 196460/654866 (30%) Split: 4  Label True: 1  Stored: 1
Load 203009/654866 (31%) Split: 4  Label True: 0  Stored: 0
Load 209558/654866 (32%) Split: 4  Label True: 1  Stored: 1
Load 216106/654866 (33%) Split: 4  Label True: 0  Stored: 0
Load 222655/654866 (34%) Split: 4  Label True: 0  Stored: 0
Load 229204/654866 (35%) Split: 4  Label True: 1  Stored: 1
Load 235752/654866 (36%) Split: 4  Label True: 0  Stored: 0
Load 242301/654866 (37%) Split: 4  Label True: 0  Stored: 0
Load 248850/654866 (38%) Split: 4  Label True: 0  Stored: 0
Load 255398/654866 (39%) Split: 4  Label True: 0  Stored: 0
Load 261947/654866 (40%) Split: 4  Label True: 0  Stored: 0
Load 268496/654866 (41%) Split: 4  Label True: 0  Stored: 0
Load 275044/654866 (42%) Split: 4  Label True: 0  Stored: 0
Load 281593/654866 (43%) Split: 4  Label True: 0  Stored: 0
Load 288142/654866 (44%) Split: 4  Label True: 0  Stored: 0
Load 294690/654866 (45%) Split: 4  Label True: 1  Stored: 1
Load 301239/654866 (46%) Split: 4  Label True: 0  Stored: 0
Load 307788/654866 (47%) Split: 4  Label True: 1  Stored: 1
Load 314336/654866 (48%) Split: 4  Label True: 0  Stored: 0
Load 320885/654866 (49%) Split: 4  Label True: 0  Stored: 0
Load 327434/654866 (50%) Split: 4  Label True: 0  Stored: 0
Load 333982/654866 (51%) Split: 4  Label True: 0  Stored: 0
Load 340531/654866 (52%) Split: 4  Label True: 1  Stored: 1
Load 347079/654866 (53%) Split: 4  Label True: 0  Stored: 0
Load 353628/654866 (54%) Split: 4  Label True: 0  Stored: 0
Load 360177/654866 (55%) Split: 4  Label True: 1  Stored: 1
Load 366725/654866 (56%) Split: 4  Label True: 0  Stored: 0
Load 373274/654866 (57%) Split: 4  Label True: 0  Stored: 0
Load 379823/654866 (58%) Split: 4  Label True: 0  Stored: 0
Load 386371/654866 (59%) Split: 4  Label True: 0  Stored: 0
Load 392920/654866 (60%) Split: 4  Label True: 0  Stored: 0
Load 399469/654866 (61%) Split: 4  Label True: 1  Stored: 1
Load 406017/654866 (62%) Split: 4  Label True: 0  Stored: 0
Load 412566/654866 (63%) Split: 4  Label True: 1  Stored: 1
Load 419115/654866 (64%) Split: 4  Label True: 1  Stored: 1
Load 425663/654866 (65%) Split: 4  Label True: 0  Stored: 0
Load 432212/654866 (66%) Split: 4  Label True: 1  Stored: 1
Load 438761/654866 (67%) Split: 4  Label True: 0  Stored: 0
Load 445309/654866 (68%) Split: 4  Label True: 0  Stored: 0
Load 451858/654866 (69%) Split: 4  Label True: 0  Stored: 0
Load 458407/654866 (70%) Split: 4  Label True: 0  Stored: 0
Load 464955/654866 (71%) Split: 4  Label True: 0  Stored: 0
Load 471504/654866 (72%) Split: 4  Label True: 1  Stored: 1
Load 478053/654866 (73%) Split: 4  Label True: 0  Stored: 0
Load 484601/654866 (74%) Split: 4  Label True: 0  Stored: 0
Load 491150/654866 (75%) Split: 4  Label True: 0  Stored: 0
Load 497699/654866 (76%) Split: 4  Label True: 0  Stored: 0
Load 504247/654866 (77%) Split: 4  Label True: 0  Stored: 0
Load 510796/654866 (78%) Split: 4  Label True: 0  Stored: 0
Load 517345/654866 (79%) Split: 4  Label True: 1  Stored: 1
Load 523893/654866 (80%) Split: 4  Label True: 0  Stored: 0
Load 530442/654866 (81%) Split: 4  Label True: 0  Stored: 0
Load 536991/654866 (82%) Split: 4  Label True: 0  Stored: 0
Load 543539/654866 (83%) Split: 4  Label True: 1  Stored: 1
Load 550088/654866 (84%) Split: 4  Label True: 0  Stored: 0
Load 556637/654866 (85%) Split: 4  Label True: 1  Stored: 1
Load 563185/654866 (86%) Split: 4  Label True: 0  Stored: 0
Load 569734/654866 (87%) Split: 4  Label True: 0  Stored: 0
Load 576283/654866 (88%) Split: 4  Label True: 0  Stored: 0
Load 582831/654866 (89%) Split: 4  Label True: 0  Stored: 0
Load 589380/654866 (90%) Split: 4  Label True: 1  Stored: 1
Load 595929/654866 (91%) Split: 4  Label True: 0  Stored: 0
Load 602477/654866 (92%) Split: 4  Label True: 0  Stored: 0
Load 609026/654866 (93%) Split: 4  Label True: 0  Stored: 0
Load 615575/654866 (94%) Split: 4  Label True: 0  Stored: 0
Load 622123/654866 (95%) Split: 4  Label True: 0  Stored: 0
Load 628672/654866 (96%) Split: 4  Label True: 1  Stored: 1
Load 635221/654866 (97%) Split: 4  Label True: 1  Stored: 1
Load 641769/654866 (98%) Split: 4  Label True: 0  Stored: 0
Load 648318/654866 (99%) Split: 4  Label True: 1  Stored: 1

Saved ./input/train_day_4.npz!
Load 6549/654866 (1%) Split: 2  Label True: 0  Stored: 0
Load 13098/654866 (2%) Split: 2  Label True: 0  Stored: 0
Load 19646/654866 (3%) Split: 2  Label True: 0  Stored: 0
Load 26195/654866 (4%) Split: 2  Label True: 0  Stored: 0
Load 32744/654866 (5%) Split: 2  Label True: 0  Stored: 0
Load 39292/654866 (6%) Split: 2  Label True: 0  Stored: 0
Load 45841/654866 (7%) Split: 2  Label True: 1  Stored: 1
Load 52390/654866 (8%) Split: 2  Label True: 1  Stored: 1
Load 58938/654866 (9%) Split: 2  Label True: 0  Stored: 0
Load 65487/654866 (10%) Split: 2  Label True: 0  Stored: 0
Load 72036/654866 (11%) Split: 2  Label True: 0  Stored: 0
Load 78584/654866 (12%) Split: 2  Label True: 0  Stored: 0
Load 85133/654866 (13%) Split: 2  Label True: 1  Stored: 1
Load 91682/654866 (14%) Split: 2  Label True: 0  Stored: 0
Load 98230/654866 (15%) Split: 2  Label True: 0  Stored: 0
Load 104779/654866 (16%) Split: 2  Label True: 0  Stored: 0
Load 111328/654866 (17%) Split: 2  Label True: 0  Stored: 0
Load 117876/654866 (18%) Split: 2  Label True: 1  Stored: 1
Load 124425/654866 (19%) Split: 2  Label True: 0  Stored: 0
Load 130974/654866 (20%) Split: 2  Label True: 0  Stored: 0
Load 137522/654866 (21%) Split: 2  Label True: 0  Stored: 0
Load 144071/654866 (22%) Split: 2  Label True: 0  Stored: 0
Load 150620/654866 (23%) Split: 2  Label True: 0  Stored: 0
Load 157168/654866 (24%) Split: 2  Label True: 0  Stored: 0
Load 163717/654866 (25%) Split: 2  Label True: 1  Stored: 1
Load 170266/654866 (26%) Split: 2  Label True: 1  Stored: 1
Load 176814/654866 (27%) Split: 2  Label True: 0  Stored: 0
Load 183363/654866 (28%) Split: 2  Label True: 0  Stored: 0
Load 189912/654866 (29%) Split: 2  Label True: 0  Stored: 0
Load 196460/654866 (30%) Split: 2  Label True: 0  Stored: 0
Load 203009/654866 (31%) Split: 2  Label True: 1  Stored: 1
Load 209558/654866 (32%) Split: 2  Label True: 0  Stored: 0
Load 216106/654866 (33%) Split: 2  Label True: 0  Stored: 0
Load 222655/654866 (34%) Split: 2  Label True: 1  Stored: 1
Load 229204/654866 (35%) Split: 2  Label True: 0  Stored: 0
Load 235752/654866 (36%) Split: 2  Label True: 0  Stored: 0
Load 242301/654866 (37%) Split: 2  Label True: 0  Stored: 0
Load 248850/654866 (38%) Split: 2  Label True: 0  Stored: 0
Load 255398/654866 (39%) Split: 2  Label True: 0  Stored: 0
Load 261947/654866 (40%) Split: 2  Label True: 0  Stored: 0
Load 268496/654866 (41%) Split: 2  Label True: 0  Stored: 0
Load 275044/654866 (42%) Split: 2  Label True: 0  Stored: 0
Load 281593/654866 (43%) Split: 2  Label True: 0  Stored: 0
Load 288142/654866 (44%) Split: 2  Label True: 0  Stored: 0
Load 294690/654866 (45%) Split: 2  Label True: 0  Stored: 0
Load 301239/654866 (46%) Split: 2  Label True: 0  Stored: 0
Load 307788/654866 (47%) Split: 2  Label True: 0  Stored: 0
Load 314336/654866 (48%) Split: 2  Label True: 1  Stored: 1
Load 320885/654866 (49%) Split: 2  Label True: 1  Stored: 1
Load 327434/654866 (50%) Split: 2  Label True: 1  Stored: 1
Load 333982/654866 (51%) Split: 2  Label True: 0  Stored: 0
Load 340531/654866 (52%) Split: 2  Label True: 0  Stored: 0
Load 347079/654866 (53%) Split: 2  Label True: 0  Stored: 0
Load 353628/654866 (54%) Split: 2  Label True: 0  Stored: 0
Load 360177/654866 (55%) Split: 2  Label True: 0  Stored: 0
Load 366725/654866 (56%) Split: 2  Label True: 0  Stored: 0
Load 373274/654866 (57%) Split: 2  Label True: 1  Stored: 1
Load 379823/654866 (58%) Split: 2  Label True: 0  Stored: 0
Load 386371/654866 (59%) Split: 2  Label True: 0  Stored: 0
Load 392920/654866 (60%) Split: 2  Label True: 0  Stored: 0
Load 399469/654866 (61%) Split: 2  Label True: 1  Stored: 1
Load 406017/654866 (62%) Split: 2  Label True: 0  Stored: 0
Load 412566/654866 (63%) Split: 2  Label True: 1  Stored: 1
Load 419115/654866 (64%) Split: 2  Label True: 0  Stored: 0
Load 425663/654866 (65%) Split: 2  Label True: 1  Stored: 1
Load 432212/654866 (66%) Split: 2  Label True: 0  Stored: 0
Load 438761/654866 (67%) Split: 2  Label True: 1  Stored: 1
Load 445309/654866 (68%) Split: 2  Label True: 0  Stored: 0
Load 451858/654866 (69%) Split: 2  Label True: 0  Stored: 0
Load 458407/654866 (70%) Split: 2  Label True: 0  Stored: 0
Load 464955/654866 (71%) Split: 2  Label True: 0  Stored: 0
Load 471504/654866 (72%) Split: 2  Label True: 0  Stored: 0
Load 478053/654866 (73%) Split: 2  Label True: 1  Stored: 1
Load 484601/654866 (74%) Split: 2  Label True: 0  Stored: 0
Load 491150/654866 (75%) Split: 2  Label True: 0  Stored: 0
Load 497699/654866 (76%) Split: 2  Label True: 0  Stored: 0
Load 504247/654866 (77%) Split: 2  Label True: 0  Stored: 0
Load 510796/654866 (78%) Split: 2  Label True: 0  Stored: 0
Load 517345/654866 (79%) Split: 2  Label True: 0  Stored: 0
Load 523893/654866 (80%) Split: 2  Label True: 0  Stored: 0
Load 530442/654866 (81%) Split: 2  Label True: 0  Stored: 0
Load 536991/654866 (82%) Split: 2  Label True: 1  Stored: 1
Load 543539/654866 (83%) Split: 2  Label True: 1  Stored: 1
Load 550088/654866 (84%) Split: 2  Label True: 0  Stored: 0
Load 556637/654866 (85%) Split: 2  Label True: 0  Stored: 0
Load 563185/654866 (86%) Split: 2  Label True: 0  Stored: 0
Load 569734/654866 (87%) Split: 2  Label True: 1  Stored: 1
Load 576283/654866 (88%) Split: 2  Label True: 0  Stored: 0
Load 582831/654866 (89%) Split: 2  Label True: 0  Stored: 0
Load 589380/654866 (90%) Split: 2  Label True: 0  Stored: 0
Load 595929/654866 (91%) Split: 2  Label True: 0  Stored: 0
Load 602477/654866 (92%) Split: 2  Label True: 0  Stored: 0
Load 609026/654866 (93%) Split: 2  Label True: 0  Stored: 0
Load 615575/654866 (94%) Split: 2  Label True: 0  Stored: 0
Load 622123/654866 (95%) Split: 2  Label True: 0  Stored: 0
Load 628672/654866 (96%) Split: 2  Label True: 0  Stored: 0
Load 635221/654866 (97%) Split: 2  Label True: 0  Stored: 0
Load 641769/654866 (98%) Split: 2  Label True: 1  Stored: 1
Load 648318/654866 (99%) Split: 2  Label True: 0  Stored: 0

Saved ./input/train_day_2.npz!
Load 6549/654866 (1%) Split: 5  Label True: 0  Stored: 0
Load 13098/654866 (2%) Split: 5  Label True: 0  Stored: 0
Load 19646/654866 (3%) Split: 5  Label True: 0  Stored: 0
Load 26195/654866 (4%) Split: 5  Label True: 0  Stored: 0
Load 32744/654866 (5%) Split: 5  Label True: 1  Stored: 1
Load 39292/654866 (6%) Split: 5  Label True: 1  Stored: 1
Load 45841/654866 (7%) Split: 5  Label True: 1  Stored: 1
Load 52390/654866 (8%) Split: 5  Label True: 0  Stored: 0
Load 58938/654866 (9%) Split: 5  Label True: 0  Stored: 0
Load 65487/654866 (10%) Split: 5  Label True: 0  Stored: 0
Load 72036/654866 (11%) Split: 5  Label True: 0  Stored: 0
Load 78584/654866 (12%) Split: 5  Label True: 0  Stored: 0
Load 85133/654866 (13%) Split: 5  Label True: 1  Stored: 1
Load 91682/654866 (14%) Split: 5  Label True: 0  Stored: 0
Load 98230/654866 (15%) Split: 5  Label True: 0  Stored: 0
Load 104779/654866 (16%) Split: 5  Label True: 0  Stored: 0
Load 111328/654866 (17%) Split: 5  Label True: 0  Stored: 0
Load 117876/654866 (18%) Split: 5  Label True: 0  Stored: 0
Load 124425/654866 (19%) Split: 5  Label True: 1  Stored: 1
Load 130974/654866 (20%) Split: 5  Label True: 0  Stored: 0
Load 137522/654866 (21%) Split: 5  Label True: 0  Stored: 0
Load 144071/654866 (22%) Split: 5  Label True: 0  Stored: 0
Load 150620/654866 (23%) Split: 5  Label True: 1  Stored: 1
Load 157168/654866 (24%) Split: 5  Label True: 0  Stored: 0
Load 163717/654866 (25%) Split: 5  Label True: 0  Stored: 0
Load 170266/654866 (26%) Split: 5  Label True: 1  Stored: 1
Load 176814/654866 (27%) Split: 5  Label True: 1  Stored: 1
Load 183363/654866 (28%) Split: 5  Label True: 1  Stored: 1
Load 189912/654866 (29%) Split: 5  Label True: 1  Stored: 1
Load 196460/654866 (30%) Split: 5  Label True: 0  Stored: 0
Load 203009/654866 (31%) Split: 5  Label True: 0  Stored: 0
Load 209558/654866 (32%) Split: 5  Label True: 0  Stored: 0
Load 216106/654866 (33%) Split: 5  Label True: 1  Stored: 1
Load 222655/654866 (34%) Split: 5  Label True: 1  Stored: 1
Load 229204/654866 (35%) Split: 5  Label True: 0  Stored: 0
Load 235752/654866 (36%) Split: 5  Label True: 0  Stored: 0
Load 242301/654866 (37%) Split: 5  Label True: 1  Stored: 1
Load 248850/654866 (38%) Split: 5  Label True: 0  Stored: 0
Load 255398/654866 (39%) Split: 5  Label True: 0  Stored: 0
Load 261947/654866 (40%) Split: 5  Label True: 0  Stored: 0
Load 268496/654866 (41%) Split: 5  Label True: 0  Stored: 0
Load 275044/654866 (42%) Split: 5  Label True: 0  Stored: 0
Load 281593/654866 (43%) Split: 5  Label True: 0  Stored: 0
Load 288142/654866 (44%) Split: 5  Label True: 0  Stored: 0
Load 294690/654866 (45%) Split: 5  Label True: 0  Stored: 0
Load 301239/654866 (46%) Split: 5  Label True: 0  Stored: 0
Load 307788/654866 (47%) Split: 5  Label True: 0  Stored: 0
Load 314336/654866 (48%) Split: 5  Label True: 0  Stored: 0
Load 320885/654866 (49%) Split: 5  Label True: 0  Stored: 0
Load 327434/654866 (50%) Split: 5  Label True: 0  Stored: 0
Load 333982/654866 (51%) Split: 5  Label True: 1  Stored: 1
Load 340531/654866 (52%) Split: 5  Label True: 1  Stored: 1
Load 347079/654866 (53%) Split: 5  Label True: 0  Stored: 0
Load 353628/654866 (54%) Split: 5  Label True: 1  Stored: 1
Load 360177/654866 (55%) Split: 5  Label True: 1  Stored: 1
Load 366725/654866 (56%) Split: 5  Label True: 0  Stored: 0
Load 373274/654866 (57%) Split: 5  Label True: 1  Stored: 1
Load 379823/654866 (58%) Split: 5  Label True: 1  Stored: 1
Load 386371/654866 (59%) Split: 5  Label True: 0  Stored: 0
Load 392920/654866 (60%) Split: 5  Label True: 0  Stored: 0
Load 399469/654866 (61%) Split: 5  Label True: 0  Stored: 0
Load 406017/654866 (62%) Split: 5  Label True: 1  Stored: 1
Load 412566/654866 (63%) Split: 5  Label True: 0  Stored: 0
Load 419115/654866 (64%) Split: 5  Label True: 0  Stored: 0
Load 425663/654866 (65%) Split: 5  Label True: 1  Stored: 1
Load 432212/654866 (66%) Split: 5  Label True: 0  Stored: 0
Load 438761/654866 (67%) Split: 5  Label True: 0  Stored: 0
Load 445309/654866 (68%) Split: 5  Label True: 0  Stored: 0
Load 451858/654866 (69%) Split: 5  Label True: 1  Stored: 1
Load 458407/654866 (70%) Split: 5  Label True: 0  Stored: 0
Load 464955/654866 (71%) Split: 5  Label True: 1  Stored: 1
Load 471504/654866 (72%) Split: 5  Label True: 1  Stored: 1
Load 478053/654866 (73%) Split: 5  Label True: 0  Stored: 0
Load 484601/654866 (74%) Split: 5  Label True: 0  Stored: 0
Load 491150/654866 (75%) Split: 5  Label True: 0  Stored: 0
Load 497699/654866 (76%) Split: 5  Label True: 1  Stored: 1
Load 504247/654866 (77%) Split: 5  Label True: 0  Stored: 0
Load 510796/654866 (78%) Split: 5  Label True: 1  Stored: 1
Load 517345/654866 (79%) Split: 5  Label True: 1  Stored: 1
Load 523893/654866 (80%) Split: 5  Label True: 0  Stored: 0
Load 530442/654866 (81%) Split: 5  Label True: 0  Stored: 0
Load 536991/654866 (82%) Split: 5  Label True: 0  Stored: 0
Load 543539/654866 (83%) Split: 5  Label True: 0  Stored: 0
Load 550088/654866 (84%) Split: 5  Label True: 1  Stored: 1
Load 556637/654866 (85%) Split: 5  Label True: 0  Stored: 0
Load 563185/654866 (86%) Split: 5  Label True: 0  Stored: 0
Load 569734/654866 (87%) Split: 5  Label True: 0  Stored: 0
Load 576283/654866 (88%) Split: 5  Label True: 0  Stored: 0
Load 582831/654866 (89%) Split: 5  Label True: 0  Stored: 0
Load 589380/654866 (90%) Split: 5  Label True: 1  Stored: 1
Load 595929/654866 (91%) Split: 5  Label True: 0  Stored: 0
Load 602477/654866 (92%) Split: 5  Label True: 0  Stored: 0
Load 609026/654866 (93%) Split: 5  Label True: 1  Stored: 1
Load 615575/654866 (94%) Split: 5  Label True: 0  Stored: 0
Load 622123/654866 (95%) Split: 5  Label True: 1  Stored: 1
Load 628672/654866 (96%) Split: 5  Label True: 0  Stored: 0
Load 635221/654866 (97%) Split: 5  Label True: 0  Stored: 0
Load 641769/654866 (98%) Split: 5  Label True: 0  Stored: 0
Load 648318/654866 (99%) Split: 5  Label True: 0  Stored: 0

Saved ./input/train_day_5.npz!
Load 6549/654865 (1%) Split: 6  Label True: 0  Stored: 0
Load 13098/654865 (2%) Split: 6  Label True: 0  Stored: 0
Load 19646/654865 (3%) Split: 6  Label True: 0  Stored: 0
Load 26195/654865 (4%) Split: 6  Label True: 0  Stored: 0
Load 32744/654865 (5%) Split: 6  Label True: 1  Stored: 1
Load 39292/654865 (6%) Split: 6  Label True: 1  Stored: 1
Load 45841/654865 (7%) Split: 6  Label True: 0  Stored: 0
Load 52390/654865 (8%) Split: 6  Label True: 0  Stored: 0
Load 58938/654865 (9%) Split: 6  Label True: 0  Stored: 0
Load 65487/654865 (10%) Split: 6  Label True: 0  Stored: 0
Load 72036/654865 (11%) Split: 6  Label True: 0  Stored: 0
Load 78584/654865 (12%) Split: 6  Label True: 0  Stored: 0
Load 85133/654865 (13%) Split: 6  Label True: 1  Stored: 1
Load 91682/654865 (14%) Split: 6  Label True: 0  Stored: 0
Load 98230/654865 (15%) Split: 6  Label True: 0  Stored: 0
Load 104779/654865 (16%) Split: 6  Label True: 0  Stored: 0
Load 111328/654865 (17%) Split: 6  Label True: 0  Stored: 0
Load 117876/654865 (18%) Split: 6  Label True: 1  Stored: 1
Load 124425/654865 (19%) Split: 6  Label True: 0  Stored: 0
Load 130974/654865 (20%) Split: 6  Label True: 0  Stored: 0
Load 137522/654865 (21%) Split: 6  Label True: 0  Stored: 0
Load 144071/654865 (22%) Split: 6  Label True: 1  Stored: 1
Load 150619/654865 (23%) Split: 6  Label True: 1  Stored: 1
Load 157168/654865 (24%) Split: 6  Label True: 0  Stored: 0
Load 163717/654865 (25%) Split: 6  Label True: 0  Stored: 0
Load 170265/654865 (26%) Split: 6  Label True: 1  Stored: 1
Load 176814/654865 (27%) Split: 6  Label True: 1  Stored: 1
Load 183363/654865 (28%) Split: 6  Label True: 1  Stored: 1
Load 189911/654865 (29%) Split: 6  Label True: 0  Stored: 0
Load 196460/654865 (30%) Split: 6  Label True: 0  Stored: 0
Load 203009/654865 (31%) Split: 6  Label True: 1  Stored: 1
Load 209557/654865 (32%) Split: 6  Label True: 0  Stored: 0
Load 216106/654865 (33%) Split: 6  Label True: 0  Stored: 0
Load 222655/654865 (34%) Split: 6  Label True: 0  Stored: 0
Load 229203/654865 (35%) Split: 6  Label True: 0  Stored: 0
Load 235752/654865 (36%) Split: 6  Label True: 0  Stored: 0
Load 242301/654865 (37%) Split: 6  Label True: 1  Stored: 1
Load 248849/654865 (38%) Split: 6  Label True: 0  Stored: 0
Load 255398/654865 (39%) Split: 6  Label True: 0  Stored: 0
Load 261947/654865 (40%) Split: 6  Label True: 0  Stored: 0
Load 268495/654865 (41%) Split: 6  Label True: 0  Stored: 0
Load 275044/654865 (42%) Split: 6  Label True: 1  Stored: 1
Load 281592/654865 (43%) Split: 6  Label True: 1  Stored: 1
Load 288141/654865 (44%) Split: 6  Label True: 0  Stored: 0
Load 294690/654865 (45%) Split: 6  Label True: 1  Stored: 1
Load 301238/654865 (46%) Split: 6  Label True: 0  Stored: 0
Load 307787/654865 (47%) Split: 6  Label True: 1  Stored: 1
Load 314336/654865 (48%) Split: 6  Label True: 0  Stored: 0
Load 320884/654865 (49%) Split: 6  Label True: 0  Stored: 0
Load 327433/654865 (50%) Split: 6  Label True: 1  Stored: 1
Load 333982/654865 (51%) Split: 6  Label True: 1  Stored: 1
Load 340530/654865 (52%) Split: 6  Label True: 0  Stored: 0
Load 347079/654865 (53%) Split: 6  Label True: 0  Stored: 0
Load 353628/654865 (54%) Split: 6  Label True: 0  Stored: 0
Load 360176/654865 (55%) Split: 6  Label True: 1  Stored: 1
Load 366725/654865 (56%) Split: 6  Label True: 0  Stored: 0
Load 373274/654865 (57%) Split: 6  Label True: 1  Stored: 1
Load 379822/654865 (58%) Split: 6  Label True: 1  Stored: 1
Load 386371/654865 (59%) Split: 6  Label True: 1  Stored: 1
Load 392920/654865 (60%) Split: 6  Label True: 0  Stored: 0
Load 399468/654865 (61%) Split: 6  Label True: 0  Stored: 0
Load 406017/654865 (62%) Split: 6  Label True: 0  Stored: 0
Load 412565/654865 (63%) Split: 6  Label True: 0  Stored: 0
Load 419114/654865 (64%) Split: 6  Label True: 1  Stored: 1
Load 425663/654865 (65%) Split: 6  Label True: 0  Stored: 0
Load 432211/654865 (66%) Split: 6  Label True: 1  Stored: 1
Load 438760/654865 (67%) Split: 6  Label True: 0  Stored: 0
Load 445309/654865 (68%) Split: 6  Label True: 1  Stored: 1
Load 451857/654865 (69%) Split: 6  Label True: 0  Stored: 0
Load 458406/654865 (70%) Split: 6  Label True: 1  Stored: 1
Load 464955/654865 (71%) Split: 6  Label True: 0  Stored: 0
Load 471503/654865 (72%) Split: 6  Label True: 1  Stored: 1
Load 478052/654865 (73%) Split: 6  Label True: 0  Stored: 0
Load 484601/654865 (74%) Split: 6  Label True: 0  Stored: 0
Load 491149/654865 (75%) Split: 6  Label True: 0  Stored: 0
Load 497698/654865 (76%) Split: 6  Label True: 0  Stored: 0
Load 504247/654865 (77%) Split: 6  Label True: 0  Stored: 0
Load 510795/654865 (78%) Split: 6  Label True: 0  Stored: 0
Load 517344/654865 (79%) Split: 6  Label True: 1  Stored: 1
Load 523893/654865 (80%) Split: 6  Label True: 1  Stored: 1
Load 530441/654865 (81%) Split: 6  Label True: 0  Stored: 0
Load 536990/654865 (82%) Split: 6  Label True: 0  Stored: 0
Load 543538/654865 (83%) Split: 6  Label True: 0  Stored: 0
Load 550087/654865 (84%) Split: 6  Label True: 0  Stored: 0
Load 556636/654865 (85%) Split: 6  Label True: 0  Stored: 0
Load 563184/654865 (86%) Split: 6  Label True: 0  Stored: 0
Load 569733/654865 (87%) Split: 6  Label True: 0  Stored: 0
Load 576282/654865 (88%) Split: 6  Label True: 1  Stored: 1
Load 582830/654865 (89%) Split: 6  Label True: 1  Stored: 1
Load 589379/654865 (90%) Split: 6  Label True: 1  Stored: 1
Load 595928/654865 (91%) Split: 6  Label True: 0  Stored: 0
Load 602476/654865 (92%) Split: 6  Label True: 0  Stored: 0
Load 609025/654865 (93%) Split: 6  Label True: 0  Stored: 0
Load 615574/654865 (94%) Split: 6  Label True: 0  Stored: 0
Load 622122/654865 (95%) Split: 6  Label True: 0  Stored: 0
Load 628671/654865 (96%) Split: 6  Label True: 0  Stored: 0
Load 635220/654865 (97%) Split: 6  Label True: 0  Stored: 0
Load 641768/654865 (98%) Split: 6  Label True: 0  Stored: 0
Load 648317/654865 (99%) Split: 6  Label True: 0  Stored: 0

Saved ./input/train_day_6.npz!
Load 6549/654866 (1%) Split: 3  Label True: 0  Stored: 0
Load 13098/654866 (2%) Split: 3  Label True: 0  Stored: 0
Load 19646/654866 (3%) Split: 3  Label True: 0  Stored: 0
Load 26195/654866 (4%) Split: 3  Label True: 0  Stored: 0
Load 32744/654866 (5%) Split: 3  Label True: 0  Stored: 0
Load 39292/654866 (6%) Split: 3  Label True: 0  Stored: 0
Load 45841/654866 (7%) Split: 3  Label True: 0  Stored: 0
Load 52390/654866 (8%) Split: 3  Label True: 0  Stored: 0
Load 58938/654866 (9%) Split: 3  Label True: 0  Stored: 0
Load 65487/654866 (10%) Split: 3  Label True: 0  Stored: 0
Load 72036/654866 (11%) Split: 3  Label True: 0  Stored: 0
Load 78584/654866 (12%) Split: 3  Label True: 0  Stored: 0
Load 85133/654866 (13%) Split: 3  Label True: 0  Stored: 0
Load 91682/654866 (14%) Split: 3  Label True: 1  Stored: 1
Load 98230/654866 (15%) Split: 3  Label True: 1  Stored: 1
Load 104779/654866 (16%) Split: 3  Label True: 0  Stored: 0
Load 111328/654866 (17%) Split: 3  Label True: 1  Stored: 1
Load 117876/654866 (18%) Split: 3  Label True: 0  Stored: 0
Load 124425/654866 (19%) Split: 3  Label True: 0  Stored: 0
Load 130974/654866 (20%) Split: 3  Label True: 1  Stored: 1
Load 137522/654866 (21%) Split: 3  Label True: 0  Stored: 0
Load 144071/654866 (22%) Split: 3  Label True: 0  Stored: 0
Load 150620/654866 (23%) Split: 3  Label True: 1  Stored: 1
Load 157168/654866 (24%) Split: 3  Label True: 1  Stored: 1
Load 163717/654866 (25%) Split: 3  Label True: 0  Stored: 0
Load 170266/654866 (26%) Split: 3  Label True: 0  Stored: 0
Load 176814/654866 (27%) Split: 3  Label True: 0  Stored: 0
Load 183363/654866 (28%) Split: 3  Label True: 0  Stored: 0
Load 189912/654866 (29%) Split: 3  Label True: 1  Stored: 1
Load 196460/654866 (30%) Split: 3  Label True: 0  Stored: 0
Load 203009/654866 (31%) Split: 3  Label True: 0  Stored: 0
Load 209558/654866 (32%) Split: 3  Label True: 0  Stored: 0
Load 216106/654866 (33%) Split: 3  Label True: 0  Stored: 0
Load 222655/654866 (34%) Split: 3  Label True: 0  Stored: 0
Load 229204/654866 (35%) Split: 3  Label True: 0  Stored: 0
Load 235752/654866 (36%) Split: 3  Label True: 1  Stored: 1
Load 242301/654866 (37%) Split: 3  Label True: 0  Stored: 0
Load 248850/654866 (38%) Split: 3  Label True: 0  Stored: 0
Load 255398/654866 (39%) Split: 3  Label True: 0  Stored: 0
Load 261947/654866 (40%) Split: 3  Label True: 0  Stored: 0
Load 268496/654866 (41%) Split: 3  Label True: 1  Stored: 1
Load 275044/654866 (42%) Split: 3  Label True: 0  Stored: 0
Load 281593/654866 (43%) Split: 3  Label True: 0  Stored: 0
Load 288142/654866 (44%) Split: 3  Label True: 0  Stored: 0
Load 294690/654866 (45%) Split: 3  Label True: 0  Stored: 0
Load 301239/654866 (46%) Split: 3  Label True: 0  Stored: 0
Load 307788/654866 (47%) Split: 3  Label True: 0  Stored: 0
Load 314336/654866 (48%) Split: 3  Label True: 0  Stored: 0
Load 320885/654866 (49%) Split: 3  Label True: 0  Stored: 0
Load 327434/654866 (50%) Split: 3  Label True: 0  Stored: 0
Load 333982/654866 (51%) Split: 3  Label True: 0  Stored: 0
Load 340531/654866 (52%) Split: 3  Label True: 1  Stored: 1
Load 347079/654866 (53%) Split: 3  Label True: 0  Stored: 0
Load 353628/654866 (54%) Split: 3  Label True: 0  Stored: 0
Load 360177/654866 (55%) Split: 3  Label True: 1  Stored: 1
Load 366725/654866 (56%) Split: 3  Label True: 0  Stored: 0
Load 373274/654866 (57%) Split: 3  Label True: 0  Stored: 0
Load 379823/654866 (58%) Split: 3  Label True: 0  Stored: 0
Load 386371/654866 (59%) Split: 3  Label True: 0  Stored: 0
Load 392920/654866 (60%) Split: 3  Label True: 0  Stored: 0
Load 399469/654866 (61%) Split: 3  Label True: 1  Stored: 1
Load 406017/654866 (62%) Split: 3  Label True: 0  Stored: 0
Load 412566/654866 (63%) Split: 3  Label True: 0  Stored: 0
Load 419115/654866 (64%) Split: 3  Label True: 0  Stored: 0
Load 425663/654866 (65%) Split: 3  Label True: 0  Stored: 0
Load 432212/654866 (66%) Split: 3  Label True: 0  Stored: 0
Load 438761/654866 (67%) Split: 3  Label True: 0  Stored: 0
Load 445309/654866 (68%) Split: 3  Label True: 0  Stored: 0
Load 451858/654866 (69%) Split: 3  Label True: 0  Stored: 0
Load 458407/654866 (70%) Split: 3  Label True: 0  Stored: 0
Load 464955/654866 (71%) Split: 3  Label True: 0  Stored: 0
Load 471504/654866 (72%) Split: 3  Label True: 0  Stored: 0
Load 478053/654866 (73%) Split: 3  Label True: 1  Stored: 1
Load 484601/654866 (74%) Split: 3  Label True: 0  Stored: 0
Load 491150/654866 (75%) Split: 3  Label True: 1  Stored: 1
Load 497699/654866 (76%) Split: 3  Label True: 0  Stored: 0
Load 504247/654866 (77%) Split: 3  Label True: 1  Stored: 1
Load 510796/654866 (78%) Split: 3  Label True: 1  Stored: 1
Load 517345/654866 (79%) Split: 3  Label True: 0  Stored: 0
Load 523893/654866 (80%) Split: 3  Label True: 0  Stored: 0
Load 530442/654866 (81%) Split: 3  Label True: 0  Stored: 0
Load 536991/654866 (82%) Split: 3  Label True: 0  Stored: 0
Load 543539/654866 (83%) Split: 3  Label True: 0  Stored: 0
Load 550088/654866 (84%) Split: 3  Label True: 0  Stored: 0
Load 556637/654866 (85%) Split: 3  Label True: 0  Stored: 0
Load 563185/654866 (86%) Split: 3  Label True: 0  Stored: 0
Load 569734/654866 (87%) Split: 3  Label True: 0  Stored: 0
Load 576283/654866 (88%) Split: 3  Label True: 0  Stored: 0
Load 582831/654866 (89%) Split: 3  Label True: 0  Stored: 0
Load 589380/654866 (90%) Split: 3  Label True: 0  Stored: 0
Load 595929/654866 (91%) Split: 3  Label True: 0  Stored: 0
Load 602477/654866 (92%) Split: 3  Label True: 0  Stored: 0
Load 609026/654866 (93%) Split: 3  Label True: 0  Stored: 0
Load 615575/654866 (94%) Split: 3  Label True: 0  Stored: 0
Load 622123/654866 (95%) Split: 3  Label True: 0  Stored: 0
Load 628672/654866 (96%) Split: 3  Label True: 1  Stored: 1
Load 635221/654866 (97%) Split: 3  Label True: 1  Stored: 1
Load 641769/654866 (98%) Split: 3  Label True: 0  Stored: 0
Load 648318/654866 (99%) Split: 3  Label True: 0  Stored: 0

Saved ./input/train_day_3.npz!
Load 6549/654866 (1%) Split: 1  Label True: 1  Stored: 1
Load 13098/654866 (2%) Split: 1  Label True: 1  Stored: 1
Load 19646/654866 (3%) Split: 1  Label True: 0  Stored: 0
Load 26195/654866 (4%) Split: 1  Label True: 0  Stored: 0
Load 32744/654866 (5%) Split: 1  Label True: 0  Stored: 0
Load 39292/654866 (6%) Split: 1  Label True: 0  Stored: 0
Load 45841/654866 (7%) Split: 1  Label True: 0  Stored: 0
Load 52390/654866 (8%) Split: 1  Label True: 0  Stored: 0
Load 58938/654866 (9%) Split: 1  Label True: 1  Stored: 1
Load 65487/654866 (10%) Split: 1  Label True: 0  Stored: 0
Load 72036/654866 (11%) Split: 1  Label True: 1  Stored: 1
Load 78584/654866 (12%) Split: 1  Label True: 0  Stored: 0
Load 85133/654866 (13%) Split: 1  Label True: 0  Stored: 0
Load 91682/654866 (14%) Split: 1  Label True: 0  Stored: 0
Load 98230/654866 (15%) Split: 1  Label True: 1  Stored: 1
Load 104779/654866 (16%) Split: 1  Label True: 0  Stored: 0
Load 111328/654866 (17%) Split: 1  Label True: 0  Stored: 0
Load 117876/654866 (18%) Split: 1  Label True: 0  Stored: 0
Load 124425/654866 (19%) Split: 1  Label True: 1  Stored: 1
Load 130974/654866 (20%) Split: 1  Label True: 1  Stored: 1
Load 137522/654866 (21%) Split: 1  Label True: 0  Stored: 0
Load 144071/654866 (22%) Split: 1  Label True: 0  Stored: 0
Load 150620/654866 (23%) Split: 1  Label True: 0  Stored: 0
Load 157168/654866 (24%) Split: 1  Label True: 1  Stored: 1
Load 163717/654866 (25%) Split: 1  Label True: 1  Stored: 1
Load 170266/654866 (26%) Split: 1  Label True: 0  Stored: 0
Load 176814/654866 (27%) Split: 1  Label True: 1  Stored: 1
Load 183363/654866 (28%) Split: 1  Label True: 0  Stored: 0
Load 189912/654866 (29%) Split: 1  Label True: 1  Stored: 1
Load 196460/654866 (30%) Split: 1  Label True: 1  Stored: 1
Load 203009/654866 (31%) Split: 1  Label True: 0  Stored: 0
Load 209558/654866 (32%) Split: 1  Label True: 0  Stored: 0
Load 216106/654866 (33%) Split: 1  Label True: 1  Stored: 1
Load 222655/654866 (34%) Split: 1  Label True: 0  Stored: 0
Load 229204/654866 (35%) Split: 1  Label True: 1  Stored: 1
Load 235752/654866 (36%) Split: 1  Label True: 0  Stored: 0
Load 242301/654866 (37%) Split: 1  Label True: 0  Stored: 0
Load 248850/654866 (38%) Split: 1  Label True: 0  Stored: 0
Load 255398/654866 (39%) Split: 1  Label True: 0  Stored: 0
Load 261947/654866 (40%) Split: 1  Label True: 1  Stored: 1
Load 268496/654866 (41%) Split: 1  Label True: 0  Stored: 0
Load 275044/654866 (42%) Split: 1  Label True: 0  Stored: 0
Load 281593/654866 (43%) Split: 1  Label True: 0  Stored: 0
Load 288142/654866 (44%) Split: 1  Label True: 0  Stored: 0
Load 294690/654866 (45%) Split: 1  Label True: 0  Stored: 0
Load 301239/654866 (46%) Split: 1  Label True: 0  Stored: 0
Load 307788/654866 (47%) Split: 1  Label True: 0  Stored: 0
Load 314336/654866 (48%) Split: 1  Label True: 0  Stored: 0
Load 320885/654866 (49%) Split: 1  Label True: 0  Stored: 0
Load 327434/654866 (50%) Split: 1  Label True: 0  Stored: 0
Load 333982/654866 (51%) Split: 1  Label True: 0  Stored: 0
Load 340531/654866 (52%) Split: 1  Label True: 0  Stored: 0
Load 347079/654866 (53%) Split: 1  Label True: 0  Stored: 0
Load 353628/654866 (54%) Split: 1  Label True: 1  Stored: 1
Load 360177/654866 (55%) Split: 1  Label True: 0  Stored: 0
Load 366725/654866 (56%) Split: 1  Label True: 0  Stored: 0
Load 373274/654866 (57%) Split: 1  Label True: 1  Stored: 1
Load 379823/654866 (58%) Split: 1  Label True: 1  Stored: 1
Load 386371/654866 (59%) Split: 1  Label True: 0  Stored: 0
Load 392920/654866 (60%) Split: 1  Label True: 0  Stored: 0
Load 399469/654866 (61%) Split: 1  Label True: 0  Stored: 0
Load 406017/654866 (62%) Split: 1  Label True: 0  Stored: 0
Load 412566/654866 (63%) Split: 1  Label True: 0  Stored: 0
Load 419115/654866 (64%) Split: 1  Label True: 0  Stored: 0
Load 425663/654866 (65%) Split: 1  Label True: 0  Stored: 0
Load 432212/654866 (66%) Split: 1  Label True: 0  Stored: 0
Load 438761/654866 (67%) Split: 1  Label True: 0  Stored: 0
Load 445309/654866 (68%) Split: 1  Label True: 0  Stored: 0
Load 451858/654866 (69%) Split: 1  Label True: 1  Stored: 1
Load 458407/654866 (70%) Split: 1  Label True: 0  Stored: 0
Load 464955/654866 (71%) Split: 1  Label True: 0  Stored: 0
Load 471504/654866 (72%) Split: 1  Label True: 0  Stored: 0
Load 478053/654866 (73%) Split: 1  Label True: 0  Stored: 0
Load 484601/654866 (74%) Split: 1  Label True: 0  Stored: 0
Load 491150/654866 (75%) Split: 1  Label True: 0  Stored: 0
Load 497699/654866 (76%) Split: 1  Label True: 0  Stored: 0
Load 504247/654866 (77%) Split: 1  Label True: 0  Stored: 0
Load 510796/654866 (78%) Split: 1  Label True: 0  Stored: 0
Load 517345/654866 (79%) Split: 1  Label True: 1  Stored: 1
Load 523893/654866 (80%) Split: 1  Label True: 0  Stored: 0
Load 530442/654866 (81%) Split: 1  Label True: 1  Stored: 1
Load 536991/654866 (82%) Split: 1  Label True: 0  Stored: 0
Load 543539/654866 (83%) Split: 1  Label True: 1  Stored: 1
Load 550088/654866 (84%) Split: 1  Label True: 1  Stored: 1
Load 556637/654866 (85%) Split: 1  Label True: 0  Stored: 0
Load 563185/654866 (86%) Split: 1  Label True: 1  Stored: 1
Load 569734/654866 (87%) Split: 1  Label True: 0  Stored: 0
Load 576283/654866 (88%) Split: 1  Label True: 1  Stored: 1
Load 582831/654866 (89%) Split: 1  Label True: 0  Stored: 0
Load 589380/654866 (90%) Split: 1  Label True: 1  Stored: 1
Load 595929/654866 (91%) Split: 1  Label True: 1  Stored: 1
Load 602477/654866 (92%) Split: 1  Label True: 0  Stored: 0
Load 609026/654866 (93%) Split: 1  Label True: 0  Stored: 0
Load 615575/654866 (94%) Split: 1  Label True: 0  Stored: 0
Load 622123/654866 (95%) Split: 1  Label True: 1  Stored: 1
Load 628672/654866 (96%) Split: 1  Label True: 0  Stored: 0
Load 635221/654866 (97%) Split: 1  Label True: 0  Stored: 0
Load 641769/654866 (98%) Split: 1  Label True: 0  Stored: 0
Load 648318/654866 (99%) Split: 1  Label True: 0  Stored: 0

Saved ./input/train_day_1.npz!
Constructing convertDicts Split: 0
Constructing convertDicts Split: 1
Constructing convertDicts Split: 2
Constructing convertDicts Split: 3
Constructing convertDicts Split: 4
Constructing convertDicts Split: 5
Constructing convertDicts Split: 6
Total number of samples: 4584061
Divided into days/splits:
 [654866, 654866, 654866, 654866, 654866, 654866, 654865]
Not existing ./input/train_day_0_processed.npz
Processed ./input/train_day_0_processed.npz
Not existing ./input/train_day_1_processed.npz
Processed ./input/train_day_1_processed.npz
Not existing ./input/train_day_2_processed.npz
Processed ./input/train_day_2_processed.npz
Not existing ./input/train_day_3_processed.npz
Processed ./input/train_day_3_processed.npz
Not existing ./input/train_day_5_processed.npz
Processed ./input/train_day_5_processed.npz
Not existing ./input/train_day_4_processed.npz
Processed ./input/train_day_4_processed.npz
Not existing ./input/train_day_6_processed.npz
Processed ./input/train_day_6_processed.npz
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
Concatenating multiple days into ./input/kaggleAdDisplayChallenge_processed.npz file
Loaded day: 0 y = 1: 167951 y = 0: 486915
Loaded day: 1 y = 1: 332435 y = 0: 977297
Loaded day: 2 y = 1: 494410 y = 0: 1470188
Loaded day: 3 y = 1: 657171 y = 0: 1962293
Loaded day: 4 y = 1: 823199 y = 0: 2451131
Loaded day: 5 y = 1: 987829 y = 0: 2941367
Loaded day: 6 y = 1: 1149931 y = 0: 3434130
Loaded counts!
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 31/31 of epoch 0, 4661.26 ms/it, loss 0.560811
Embedding lookup time: 66.49 ms, MLP time: 935.99 ms, interaction time: 715.69 ms 
Running dlrm: 1379.19 ms, backpropagation after dlrm: 3279.15 ms, backpass: 2844.43 ms
The MLP time is 29.01566457748413
The embedding time is 2.061169385910034
The interaction time is 22.186250686645508
The total time is 363.41268134117126
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 4421.33 ms/it, loss 0.560811
Embedding lookup time: 46.19 ms, MLP time: 907.28 ms, interaction time: 703.53 ms 
Running dlrm: 1323.82 ms, backpropagation after dlrm: 3094.63 ms, backpass: 2691.05 ms
The MLP time is 28.125537395477295
The embedding time is 1.431952953338623
The interaction time is 21.809555530548096
The total time is 356.1551704406738
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 10
Finished training it 31/31 of epoch 0, 2760.80 ms/it, loss 0.560811
Embedding lookup time: 46.71 ms, MLP time: 542.30 ms, interaction time: 408.22 ms 
Running dlrm: 812.17 ms, backpropagation after dlrm: 1945.62 ms, backpass: 1502.72 ms
The MLP time is 16.8112051486969
The embedding time is 1.448080062866211
The interaction time is 12.654812097549438
The total time is 309.57455587387085
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2759.66 ms/it, loss 0.560811
Embedding lookup time: 49.16 ms, MLP time: 547.24 ms, interaction time: 404.39 ms 
Running dlrm: 817.78 ms, backpropagation after dlrm: 1938.73 ms, backpass: 1509.30 ms
The MLP time is 16.964328289031982
The embedding time is 1.5240473747253418
The interaction time is 12.536226987838745
The total time is 305.12869215011597
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 15
Finished training it 31/31 of epoch 0, 2525.31 ms/it, loss 0.560811
Embedding lookup time: 44.75 ms, MLP time: 425.18 ms, interaction time: 311.67 ms 
Running dlrm: 644.96 ms, backpropagation after dlrm: 1876.54 ms, backpass: 1445.58 ms
The MLP time is 13.180708169937134
The embedding time is 1.3873703479766846
The interaction time is 9.661654233932495
The total time is 297.539671421051
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2511.39 ms/it, loss 0.560811
Embedding lookup time: 43.97 ms, MLP time: 423.29 ms, interaction time: 306.62 ms 
Running dlrm: 639.80 ms, backpropagation after dlrm: 1868.26 ms, backpass: 1431.02 ms
The MLP time is 13.121917486190796
The embedding time is 1.3631024360656738
The interaction time is 9.505333185195923
The total time is 298.02420020103455
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2029.15 ms/it, loss 0.560811
Embedding lookup time: 44.59 ms, MLP time: 381.30 ms, interaction time: 271.41 ms 
Running dlrm: 582.84 ms, backpropagation after dlrm: 1442.85 ms, backpass: 1058.17 ms
The MLP time is 11.820452451705933
The embedding time is 1.3822765350341797
The interaction time is 8.413776874542236
The total time is 288.33924102783203
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2031.84 ms/it, loss 0.560811
Embedding lookup time: 40.58 ms, MLP time: 381.80 ms, interaction time: 276.94 ms 
Running dlrm: 580.56 ms, backpropagation after dlrm: 1448.61 ms, backpass: 1028.41 ms
The MLP time is 11.835833549499512
The embedding time is 1.2578637599945068
The interaction time is 8.585265159606934
The total time is 281.7130403518677
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2036.07 ms/it, loss 0.560811
Embedding lookup time: 40.01 ms, MLP time: 366.30 ms, interaction time: 266.51 ms 
Running dlrm: 558.92 ms, backpropagation after dlrm: 1474.22 ms, backpass: 1044.69 ms
The MLP time is 11.355299711227417
The embedding time is 1.2402393817901611
The interaction time is 8.261898756027222
The total time is 283.5230522155762
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2037.09 ms/it, loss 0.560811
Embedding lookup time: 43.08 ms, MLP time: 379.24 ms, interaction time: 271.22 ms 
Running dlrm: 577.95 ms, backpropagation after dlrm: 1456.54 ms, backpass: 1034.64 ms
The MLP time is 11.756335020065308
The embedding time is 1.3354613780975342
The interaction time is 8.407895565032959
The total time is 283.18028020858765
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2030.87 ms/it, loss 0.560811
Embedding lookup time: 39.18 ms, MLP time: 372.96 ms, interaction time: 270.49 ms 
Running dlrm: 566.69 ms, backpropagation after dlrm: 1461.60 ms, backpass: 1036.84 ms
The MLP time is 11.561782836914062
The embedding time is 1.2145743370056152
The interaction time is 8.385136842727661
The total time is 282.53734040260315
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2041.91 ms/it, loss 0.560811
Embedding lookup time: 41.57 ms, MLP time: 387.43 ms, interaction time: 272.25 ms 
Running dlrm: 586.05 ms, backpropagation after dlrm: 1452.74 ms, backpass: 1067.22 ms
The MLP time is 12.010263681411743
The embedding time is 1.2887921333312988
The interaction time is 8.439858198165894
The total time is 286.36694598197937
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 55, in <module>
    import torch.jit
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/__init__.py", line 1798, in <module>
    from torch import storage as storage  # usort: skip
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/storage.py", line 36, in <module>
    import numpy as np
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/numpy/__init__.py", line 114, in <module>
    from numpy.__config__ import show_config
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/numpy/__config__.py", line 4, in <module>
    from numpy._core._multiarray_umath import (
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/numpy/_core/__init__.py", line 23, in <module>
    from . import multiarray
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/numpy/_core/multiarray.py", line 10, in <module>
    from . import overrides
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/numpy/_core/overrides.py", line 7, in <module>
    from numpy._core._multiarray_umath import (
RuntimeError: CPU dispatcher tracer already initlized
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 8192/30697 of epoch 0, 17.42 ms/it, loss 0.486029
Embedding lookup time: 3.15 ms, MLP time: 2.05 ms, interaction time: 2.64 ms 
Running dlrm: 6.82 ms, backpropagation after dlrm: 10.21 ms, backpass: 8.34 ms
Finished training it 16384/30697 of epoch 0, 17.32 ms/it, loss 0.464877
Embedding lookup time: 3.13 ms, MLP time: 2.03 ms, interaction time: 2.63 ms 
Running dlrm: 6.76 ms, backpropagation after dlrm: 10.16 ms, backpass: 8.30 ms
Finished training it 24576/30697 of epoch 0, 17.31 ms/it, loss 0.459999
Embedding lookup time: 3.13 ms, MLP time: 2.03 ms, interaction time: 2.63 ms 
Running dlrm: 6.77 ms, backpropagation after dlrm: 10.15 ms, backpass: 8.29 ms
Finished training it 30697/30697 of epoch 0, 17.31 ms/it, loss 0.457078
Embedding lookup time: 3.13 ms, MLP time: 2.03 ms, interaction time: 2.63 ms 
Running dlrm: 6.77 ms, backpropagation after dlrm: 10.16 ms, backpass: 8.29 ms
The MLP time is 62.50965356826782
The embedding time is 96.15705251693726
The interaction time is 80.8440511226654
The total time is 771.5565257072449
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 17.05 ms/it, loss 0.486029
Embedding lookup time: 2.70 ms, MLP time: 2.05 ms, interaction time: 2.62 ms 
Running dlrm: 6.32 ms, backpropagation after dlrm: 10.33 ms, backpass: 8.45 ms
Finished training it 16384/30697 of epoch 0, 16.96 ms/it, loss 0.464877
Embedding lookup time: 2.69 ms, MLP time: 2.04 ms, interaction time: 2.62 ms 
Running dlrm: 6.30 ms, backpropagation after dlrm: 10.27 ms, backpass: 8.40 ms
Finished training it 24576/30697 of epoch 0, 16.97 ms/it, loss 0.459999
Embedding lookup time: 2.69 ms, MLP time: 2.04 ms, interaction time: 2.62 ms 
Running dlrm: 6.30 ms, backpropagation after dlrm: 10.28 ms, backpass: 8.40 ms
Finished training it 30697/30697 of epoch 0, 16.97 ms/it, loss 0.457078
Embedding lookup time: 2.69 ms, MLP time: 2.04 ms, interaction time: 2.62 ms 
Running dlrm: 6.30 ms, backpropagation after dlrm: 10.28 ms, backpass: 8.40 ms
The MLP time is 62.66624927520752
The embedding time is 82.59607744216919
The interaction time is 80.43879461288452
The total time is 761.2715737819672
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 10
Finished training it 8192/30697 of epoch 0, 16.24 ms/it, loss 0.485887
Embedding lookup time: 3.21 ms, MLP time: 1.66 ms, interaction time: 2.54 ms 
Running dlrm: 6.44 ms, backpropagation after dlrm: 9.41 ms, backpass: 7.55 ms
Finished training it 16384/30697 of epoch 0, 16.23 ms/it, loss 0.464886
Embedding lookup time: 3.22 ms, MLP time: 1.65 ms, interaction time: 2.53 ms 
Running dlrm: 6.44 ms, backpropagation after dlrm: 9.40 ms, backpass: 7.55 ms
Finished training it 24576/30697 of epoch 0, 16.22 ms/it, loss 0.460119
Embedding lookup time: 3.22 ms, MLP time: 1.65 ms, interaction time: 2.53 ms 
Running dlrm: 6.44 ms, backpropagation after dlrm: 9.39 ms, backpass: 7.54 ms
Finished training it 30697/30697 of epoch 0, 16.14 ms/it, loss 0.457172
Embedding lookup time: 3.19 ms, MLP time: 1.64 ms, interaction time: 2.53 ms 
Running dlrm: 6.39 ms, backpropagation after dlrm: 9.35 ms, backpass: 7.51 ms
The MLP time is 50.72923755645752
The embedding time is 98.6498441696167
The interaction time is 77.70760345458984
The total time is 732.6577215194702
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 15.97 ms/it, loss 0.485887
Embedding lookup time: 2.79 ms, MLP time: 1.69 ms, interaction time: 2.53 ms 
Running dlrm: 6.02 ms, backpropagation after dlrm: 9.56 ms, backpass: 7.69 ms
Finished training it 16384/30697 of epoch 0, 15.77 ms/it, loss 0.464886
Embedding lookup time: 2.76 ms, MLP time: 1.67 ms, interaction time: 2.51 ms 
Running dlrm: 5.95 ms, backpropagation after dlrm: 9.43 ms, backpass: 7.58 ms
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 2111, in <module>
    run()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 1785, in run
    E.backward()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 31/31 of epoch 0, 4716.44 ms/it, loss 0.560811
Embedding lookup time: 60.23 ms, MLP time: 942.64 ms, interaction time: 744.08 ms 
Running dlrm: 1394.05 ms, backpropagation after dlrm: 3319.50 ms, backpass: 2866.29 ms
The MLP time is 29.22169518470764
The embedding time is 1.8671057224273682
The interaction time is 23.066452741622925
The total time is 368.7018096446991
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 4423.45 ms/it, loss 0.560811
Embedding lookup time: 45.32 ms, MLP time: 907.29 ms, interaction time: 705.39 ms 
Running dlrm: 1323.98 ms, backpropagation after dlrm: 3096.56 ms, backpass: 2691.99 ms
The MLP time is 28.125940799713135
The embedding time is 1.4050488471984863
The interaction time is 21.866952419281006
The total time is 355.1596612930298
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 10
Finished training it 31/31 of epoch 0, 2800.64 ms/it, loss 0.560811
Embedding lookup time: 89.27 ms, MLP time: 550.46 ms, interaction time: 435.71 ms 
Running dlrm: 877.04 ms, backpropagation after dlrm: 1920.86 ms, backpass: 1508.46 ms
The MLP time is 17.064287424087524
The embedding time is 2.7673699855804443
The interaction time is 13.507156133651733
The total time is 311.3413076400757
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2761.73 ms/it, loss 0.560811
Embedding lookup time: 47.64 ms, MLP time: 545.97 ms, interaction time: 413.06 ms 
Running dlrm: 819.30 ms, backpropagation after dlrm: 1939.22 ms, backpass: 1517.29 ms
The MLP time is 16.92496633529663
The embedding time is 1.476719856262207
The interaction time is 12.804875373840332
The total time is 307.2052686214447
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 15
Finished training it 31/31 of epoch 0, 2551.83 ms/it, loss 0.560811
Embedding lookup time: 57.88 ms, MLP time: 425.72 ms, interaction time: 334.95 ms 
Running dlrm: 670.63 ms, backpropagation after dlrm: 1878.65 ms, backpass: 1484.27 ms
The MLP time is 13.197408676147461
The embedding time is 1.7941522598266602
The interaction time is 10.383376121520996
The total time is 301.17471504211426
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2532.75 ms/it, loss 0.560811
Embedding lookup time: 42.72 ms, MLP time: 426.87 ms, interaction time: 313.20 ms 
Running dlrm: 645.97 ms, backpropagation after dlrm: 1883.70 ms, backpass: 1445.15 ms
The MLP time is 13.23300051689148
The embedding time is 1.3243331909179688
The interaction time is 9.709103107452393
The total time is 299.6492962837219
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2062.60 ms/it, loss 0.560811
Embedding lookup time: 52.44 ms, MLP time: 379.28 ms, interaction time: 312.99 ms 
Running dlrm: 607.77 ms, backpropagation after dlrm: 1452.26 ms, backpass: 1031.44 ms
The MLP time is 11.75757098197937
The embedding time is 1.6257014274597168
The interaction time is 9.702585697174072
The total time is 284.35003089904785
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 1996.16 ms/it, loss 0.560811
Embedding lookup time: 39.03 ms, MLP time: 370.31 ms, interaction time: 269.66 ms 
Running dlrm: 563.46 ms, backpropagation after dlrm: 1429.82 ms, backpass: 1026.18 ms
The MLP time is 11.479724168777466
The embedding time is 1.2100844383239746
The interaction time is 8.359401941299438
The total time is 279.9705331325531
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2101.61 ms/it, loss 0.560811
Embedding lookup time: 75.92 ms, MLP time: 384.25 ms, interaction time: 305.72 ms 
Running dlrm: 632.74 ms, backpropagation after dlrm: 1465.98 ms, backpass: 1042.91 ms
The MLP time is 11.911902666091919
The embedding time is 2.3535165786743164
The interaction time is 9.477217674255371
The total time is 287.8154091835022
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2043.69 ms/it, loss 0.560811
Embedding lookup time: 41.20 ms, MLP time: 378.78 ms, interaction time: 271.17 ms 
Running dlrm: 574.88 ms, backpropagation after dlrm: 1465.51 ms, backpass: 1043.79 ms
The MLP time is 11.74204683303833
The embedding time is 1.2770991325378418
The interaction time is 8.40637731552124
The total time is 284.2226836681366
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2148.08 ms/it, loss 0.560811
Embedding lookup time: 79.13 ms, MLP time: 404.12 ms, interaction time: 331.45 ms 
Running dlrm: 670.21 ms, backpropagation after dlrm: 1475.27 ms, backpass: 1084.77 ms
The MLP time is 12.527766704559326
The embedding time is 2.452924966812134
The interaction time is 10.27510380744934
The total time is 293.5455825328827
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 1990.94 ms/it, loss 0.560811
Embedding lookup time: 42.84 ms, MLP time: 372.43 ms, interaction time: 266.39 ms 
Running dlrm: 567.77 ms, backpropagation after dlrm: 1420.58 ms, backpass: 1025.23 ms
The MLP time is 11.545396566390991
The embedding time is 1.3279409408569336
The interaction time is 8.258230447769165
The total time is 282.3844347000122
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 8192/30697 of epoch 0, 33.31 ms/it, loss 0.486029
Embedding lookup time: 13.64 ms, MLP time: 2.38 ms, interaction time: 3.95 ms 
Running dlrm: 18.49 ms, backpropagation after dlrm: 14.27 ms, backpass: 11.74 ms
Finished training it 16384/30697 of epoch 0, 33.32 ms/it, loss 0.464877
Embedding lookup time: 13.36 ms, MLP time: 2.40 ms, interaction time: 4.05 ms 
Running dlrm: 18.30 ms, backpropagation after dlrm: 14.46 ms, backpass: 11.91 ms
Finished training it 24576/30697 of epoch 0, 33.41 ms/it, loss 0.459999
Embedding lookup time: 13.47 ms, MLP time: 2.40 ms, interaction time: 4.04 ms 
Running dlrm: 18.40 ms, backpropagation after dlrm: 14.46 ms, backpass: 11.91 ms
Finished training it 30697/30697 of epoch 0, 32.75 ms/it, loss 0.457078
Embedding lookup time: 13.09 ms, MLP time: 2.37 ms, interaction time: 3.92 ms 
Running dlrm: 17.93 ms, backpropagation after dlrm: 14.28 ms, backpass: 11.76 ms
The MLP time is 73.26973128318787
The embedding time is 411.6759741306305
The interaction time is 122.60621547698975
The total time is 1291.1779117584229
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 17.11 ms/it, loss 0.486029
Embedding lookup time: 2.75 ms, MLP time: 2.05 ms, interaction time: 2.62 ms 
Running dlrm: 6.38 ms, backpropagation after dlrm: 10.33 ms, backpass: 8.45 ms
Finished training it 16384/30697 of epoch 0, 16.89 ms/it, loss 0.464877
Embedding lookup time: 2.72 ms, MLP time: 2.03 ms, interaction time: 2.61 ms 
Running dlrm: 6.32 ms, backpropagation after dlrm: 10.19 ms, backpass: 8.32 ms
Finished training it 24576/30697 of epoch 0, 16.92 ms/it, loss 0.459999
Embedding lookup time: 2.72 ms, MLP time: 2.03 ms, interaction time: 2.61 ms 
Running dlrm: 6.32 ms, backpropagation after dlrm: 10.21 ms, backpass: 8.34 ms
Finished training it 30697/30697 of epoch 0, 16.88 ms/it, loss 0.457078
Embedding lookup time: 2.71 ms, MLP time: 2.03 ms, interaction time: 2.60 ms 
Running dlrm: 6.30 ms, backpropagation after dlrm: 10.19 ms, backpass: 8.33 ms
The MLP time is 62.457881450653076
The embedding time is 83.74703693389893
The interaction time is 80.15231704711914
The total time is 758.2185473442078
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 10
Finished training it 8192/30697 of epoch 0, 28.71 ms/it, loss 0.485887
Embedding lookup time: 12.74 ms, MLP time: 1.79 ms, interaction time: 2.90 ms 
Running dlrm: 16.42 ms, backpropagation after dlrm: 11.83 ms, backpass: 9.62 ms
Finished training it 16384/30697 of epoch 0, 28.68 ms/it, loss 0.464886
Embedding lookup time: 12.83 ms, MLP time: 1.79 ms, interaction time: 2.89 ms 
Running dlrm: 16.50 ms, backpropagation after dlrm: 11.72 ms, backpass: 9.54 ms
Finished training it 24576/30697 of epoch 0, 28.83 ms/it, loss 0.460119
Embedding lookup time: 12.82 ms, MLP time: 1.78 ms, interaction time: 2.92 ms 
Running dlrm: 16.49 ms, backpropagation after dlrm: 11.88 ms, backpass: 9.65 ms
Finished training it 30697/30697 of epoch 0, 29.01 ms/it, loss 0.457172
Embedding lookup time: 12.95 ms, MLP time: 1.79 ms, interaction time: 2.92 ms 
Running dlrm: 16.64 ms, backpropagation after dlrm: 11.91 ms, backpass: 9.67 ms
The MLP time is 54.826098918914795
The embedding time is 393.77818512916565
The interaction time is 89.24690341949463
The total time is 1132.6364786624908
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 16.00 ms/it, loss 0.485887
Embedding lookup time: 2.79 ms, MLP time: 1.69 ms, interaction time: 2.55 ms 
Running dlrm: 6.03 ms, backpropagation after dlrm: 9.58 ms, backpass: 7.70 ms
Finished training it 16384/30697 of epoch 0, 15.82 ms/it, loss 0.464886
Embedding lookup time: 2.76 ms, MLP time: 1.67 ms, interaction time: 2.53 ms 
Running dlrm: 5.96 ms, backpropagation after dlrm: 9.46 ms, backpass: 7.60 ms
Finished training it 24576/30697 of epoch 0, 15.88 ms/it, loss 0.460119
Embedding lookup time: 2.77 ms, MLP time: 1.67 ms, interaction time: 2.54 ms 
Running dlrm: 5.98 ms, backpropagation after dlrm: 9.51 ms, backpass: 7.64 ms
Finished training it 30697/30697 of epoch 0, 15.83 ms/it, loss 0.457172
Embedding lookup time: 2.76 ms, MLP time: 1.67 ms, interaction time: 2.54 ms 
Running dlrm: 5.97 ms, backpropagation after dlrm: 9.47 ms, backpass: 7.61 ms
The MLP time is 51.437942028045654
The embedding time is 85.03163647651672
The interaction time is 77.99721884727478
The total time is 722.0081648826599
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 15
Finished training it 8192/30697 of epoch 0, 37.06 ms/it, loss 0.485913
Embedding lookup time: 23.40 ms, MLP time: 1.83 ms, interaction time: 2.60 ms 
Running dlrm: 26.95 ms, backpropagation after dlrm: 9.69 ms, backpass: 7.97 ms
Finished training it 16384/30697 of epoch 0, 37.34 ms/it, loss 0.464865
Embedding lookup time: 23.61 ms, MLP time: 1.83 ms, interaction time: 2.58 ms 
Running dlrm: 27.15 ms, backpropagation after dlrm: 9.78 ms, backpass: 8.04 ms
Finished training it 24576/30697 of epoch 0, 37.49 ms/it, loss 0.460121
Embedding lookup time: 23.78 ms, MLP time: 1.83 ms, interaction time: 2.57 ms 
Running dlrm: 27.32 ms, backpropagation after dlrm: 9.75 ms, backpass: 8.02 ms
Finished training it 30697/30697 of epoch 0, 37.86 ms/it, loss 0.457142
Embedding lookup time: 23.99 ms, MLP time: 1.84 ms, interaction time: 2.63 ms 
Running dlrm: 27.57 ms, backpropagation after dlrm: 9.87 ms, backpass: 8.12 ms
The MLP time is 56.29469633102417
The embedding time is 726.7713584899902
The interaction time is 79.52438712120056
The total time is 1303.7349545955658
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 13.27 ms/it, loss 0.485887
Embedding lookup time: 1.98 ms, MLP time: 1.71 ms, interaction time: 1.80 ms 
Running dlrm: 4.87 ms, backpropagation after dlrm: 7.99 ms, backpass: 6.51 ms
Finished training it 16384/30697 of epoch 0, 13.15 ms/it, loss 0.464886
Embedding lookup time: 1.97 ms, MLP time: 1.69 ms, interaction time: 1.79 ms 
Running dlrm: 4.85 ms, backpropagation after dlrm: 7.90 ms, backpass: 6.42 ms
Finished training it 24576/30697 of epoch 0, 13.16 ms/it, loss 0.460119
Embedding lookup time: 1.98 ms, MLP time: 1.70 ms, interaction time: 1.79 ms 
Running dlrm: 4.86 ms, backpropagation after dlrm: 7.91 ms, backpass: 6.42 ms
Finished training it 30697/30697 of epoch 0, 13.20 ms/it, loss 0.457172
Embedding lookup time: 1.98 ms, MLP time: 1.70 ms, interaction time: 1.80 ms 
Running dlrm: 4.88 ms, backpropagation after dlrm: 7.92 ms, backpass: 6.44 ms
The MLP time is 52.16906452178955
The embedding time is 60.72323656082153
The interaction time is 54.99604105949402
The total time is 532.2763557434082
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 8192/30697 of epoch 0, 39.42 ms/it, loss 0.485913
Embedding lookup time: 24.52 ms, MLP time: 1.76 ms, interaction time: 3.54 ms 
Running dlrm: 28.49 ms, backpropagation after dlrm: 10.48 ms, backpass: 8.63 ms
Finished training it 16384/30697 of epoch 0, 39.13 ms/it, loss 0.464865
Embedding lookup time: 24.36 ms, MLP time: 1.74 ms, interaction time: 3.50 ms 
Running dlrm: 28.30 ms, backpropagation after dlrm: 10.40 ms, backpass: 8.56 ms
Finished training it 24576/30697 of epoch 0, 38.98 ms/it, loss 0.460121
Embedding lookup time: 24.25 ms, MLP time: 1.74 ms, interaction time: 3.44 ms 
Running dlrm: 28.16 ms, backpropagation after dlrm: 10.38 ms, backpass: 8.54 ms
Finished training it 30697/30697 of epoch 0, 39.30 ms/it, loss 0.457142
Embedding lookup time: 24.56 ms, MLP time: 1.74 ms, interaction time: 3.42 ms 
Running dlrm: 28.45 ms, backpropagation after dlrm: 10.41 ms, backpass: 8.56 ms
The MLP time is 53.506049394607544
The embedding time is 749.4428598880768
The interaction time is 106.72146248817444
The total time is 1359.7257890701294
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 13.29 ms/it, loss 0.485887
Embedding lookup time: 2.02 ms, MLP time: 1.59 ms, interaction time: 1.80 ms 
Running dlrm: 4.85 ms, backpropagation after dlrm: 8.00 ms, backpass: 6.47 ms
Finished training it 16384/30697 of epoch 0, 13.19 ms/it, loss 0.464886
Embedding lookup time: 2.02 ms, MLP time: 1.58 ms, interaction time: 1.79 ms 
Running dlrm: 4.83 ms, backpropagation after dlrm: 7.92 ms, backpass: 6.40 ms
Finished training it 24576/30697 of epoch 0, 13.20 ms/it, loss 0.460119
Embedding lookup time: 2.01 ms, MLP time: 1.58 ms, interaction time: 1.80 ms 
Running dlrm: 4.83 ms, backpropagation after dlrm: 7.94 ms, backpass: 6.41 ms
Finished training it 30697/30697 of epoch 0, 13.14 ms/it, loss 0.457172
Embedding lookup time: 2.01 ms, MLP time: 1.57 ms, interaction time: 1.79 ms 
Running dlrm: 4.81 ms, backpropagation after dlrm: 7.89 ms, backpass: 6.38 ms
The MLP time is 48.55541443824768
The embedding time is 61.925639629364014
The interaction time is 55.126113176345825
The total time is 533.9617607593536
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 8192/30697 of epoch 0, 39.01 ms/it, loss 0.485913
Embedding lookup time: 24.20 ms, MLP time: 1.74 ms, interaction time: 3.52 ms 
Running dlrm: 28.14 ms, backpropagation after dlrm: 10.43 ms, backpass: 8.60 ms
Finished training it 16384/30697 of epoch 0, 39.08 ms/it, loss 0.464865
Embedding lookup time: 24.31 ms, MLP time: 1.73 ms, interaction time: 3.50 ms 
Running dlrm: 28.24 ms, backpropagation after dlrm: 10.40 ms, backpass: 8.57 ms
Finished training it 24576/30697 of epoch 0, 40.52 ms/it, loss 0.460121
Embedding lookup time: 25.54 ms, MLP time: 1.76 ms, interaction time: 3.38 ms 
Running dlrm: 29.42 ms, backpropagation after dlrm: 10.66 ms, backpass: 8.80 ms
Finished training it 30697/30697 of epoch 0, 41.56 ms/it, loss 0.457142
Embedding lookup time: 26.44 ms, MLP time: 1.75 ms, interaction time: 3.34 ms 
Running dlrm: 30.29 ms, backpropagation after dlrm: 10.82 ms, backpass: 8.97 ms
The MLP time is 53.5280282497406
The embedding time is 768.4049198627472
The interaction time is 105.61011862754822
The total time is 1385.8092601299286
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 13.31 ms/it, loss 0.485887
Embedding lookup time: 2.02 ms, MLP time: 1.59 ms, interaction time: 1.80 ms 
Running dlrm: 4.84 ms, backpropagation after dlrm: 8.03 ms, backpass: 6.51 ms
Finished training it 16384/30697 of epoch 0, 13.19 ms/it, loss 0.464886
Embedding lookup time: 2.00 ms, MLP time: 1.57 ms, interaction time: 1.79 ms 
Running dlrm: 4.81 ms, backpropagation after dlrm: 7.95 ms, backpass: 6.45 ms
Finished training it 24576/30697 of epoch 0, 13.13 ms/it, loss 0.460119
Embedding lookup time: 2.00 ms, MLP time: 1.57 ms, interaction time: 1.79 ms 
Running dlrm: 4.79 ms, backpropagation after dlrm: 7.91 ms, backpass: 6.41 ms
Finished training it 30697/30697 of epoch 0, 13.17 ms/it, loss 0.457172
Embedding lookup time: 1.99 ms, MLP time: 1.57 ms, interaction time: 1.80 ms 
Running dlrm: 4.79 ms, backpropagation after dlrm: 7.95 ms, backpass: 6.44 ms
The MLP time is 48.40670967102051
The embedding time is 61.44677257537842
The interaction time is 55.09772062301636
The total time is 533.3170654773712
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 8192/30697 of epoch 0, 39.14 ms/it, loss 0.485913
Embedding lookup time: 24.36 ms, MLP time: 1.74 ms, interaction time: 3.43 ms 
Running dlrm: 28.26 ms, backpropagation after dlrm: 10.45 ms, backpass: 8.59 ms
Finished training it 16384/30697 of epoch 0, 39.21 ms/it, loss 0.464865
Embedding lookup time: 24.42 ms, MLP time: 1.73 ms, interaction time: 3.44 ms 
Running dlrm: 28.32 ms, backpropagation after dlrm: 10.46 ms, backpass: 8.60 ms
Finished training it 24576/30697 of epoch 0, 39.36 ms/it, loss 0.460121
Embedding lookup time: 24.60 ms, MLP time: 1.74 ms, interaction time: 3.38 ms 
Running dlrm: 28.47 ms, backpropagation after dlrm: 10.44 ms, backpass: 8.58 ms
Finished training it 30697/30697 of epoch 0, 39.15 ms/it, loss 0.457142
Embedding lookup time: 24.40 ms, MLP time: 1.73 ms, interaction time: 3.39 ms 
Running dlrm: 28.28 ms, backpropagation after dlrm: 10.43 ms, backpass: 8.58 ms
The MLP time is 53.29118728637695
The embedding time is 750.4420320987701
The interaction time is 104.69166803359985
The total time is 1361.3527903556824
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 13.30 ms/it, loss 0.485887
Embedding lookup time: 2.03 ms, MLP time: 1.58 ms, interaction time: 1.81 ms 
Running dlrm: 4.85 ms, backpropagation after dlrm: 8.01 ms, backpass: 6.50 ms
Finished training it 16384/30697 of epoch 0, 13.61 ms/it, loss 0.464886
Embedding lookup time: 2.04 ms, MLP time: 1.60 ms, interaction time: 1.81 ms 
Running dlrm: 4.88 ms, backpropagation after dlrm: 8.30 ms, backpass: 6.76 ms
Finished training it 24576/30697 of epoch 0, 14.06 ms/it, loss 0.460119
Embedding lookup time: 2.09 ms, MLP time: 1.61 ms, interaction time: 1.83 ms 
Running dlrm: 4.95 ms, backpropagation after dlrm: 8.68 ms, backpass: 7.13 ms
Finished training it 30697/30697 of epoch 0, 13.44 ms/it, loss 0.457172
Embedding lookup time: 2.02 ms, MLP time: 1.57 ms, interaction time: 1.80 ms 
Running dlrm: 4.82 ms, backpropagation after dlrm: 8.19 ms, backpass: 6.67 ms
The MLP time is 48.84072971343994
The embedding time is 62.83446168899536
The interaction time is 55.637287855148315
The total time is 547.3215410709381
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

above is disable intra op:

===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 31/31 of epoch 0, 4689.14 ms/it, loss 0.560811
Embedding lookup time: 141.51 ms, MLP time: 916.12 ms, interaction time: 748.46 ms 
Running dlrm: 1452.39 ms, backpropagation after dlrm: 3234.03 ms, backpass: 2794.23 ms
The MLP time is 28.399780750274658
The embedding time is 4.386682510375977
The interaction time is 23.202208280563354
The total time is 373.3942594528198
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 6466.48 ms/it, loss 0.560811
Embedding lookup time: 88.93 ms, MLP time: 1252.38 ms, interaction time: 1092.80 ms 
Running dlrm: 1925.53 ms, backpropagation after dlrm: 4535.53 ms, backpass: 3712.92 ms
The MLP time is 38.8237509727478
The embedding time is 2.7567684650421143
The interaction time is 33.8767466545105
The total time is 655.6437845230103
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 10
Finished training it 31/31 of epoch 0, 3389.68 ms/it, loss 0.560811
Embedding lookup time: 166.67 ms, MLP time: 660.28 ms, interaction time: 592.16 ms 
Running dlrm: 1143.03 ms, backpropagation after dlrm: 2242.13 ms, backpass: 1791.56 ms
The MLP time is 20.468756437301636
The embedding time is 5.166914701461792
The interaction time is 18.356810092926025
The total time is 332.1454071998596
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 3238.65 ms/it, loss 0.560811
Embedding lookup time: 52.45 ms, MLP time: 668.04 ms, interaction time: 556.36 ms 
Running dlrm: 1019.46 ms, backpropagation after dlrm: 2216.15 ms, backpass: 1801.18 ms
The MLP time is 20.70929455757141
The embedding time is 1.6259970664978027
The interaction time is 17.247047185897827
The total time is 336.91598677635193
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 15
Finished training it 31/31 of epoch 0, 3199.60 ms/it, loss 0.560811
Embedding lookup time: 218.24 ms, MLP time: 507.43 ms, interaction time: 479.38 ms 
Running dlrm: 985.87 ms, backpropagation after dlrm: 2209.08 ms, backpass: 1752.86 ms
The MLP time is 15.730358123779297
The embedding time is 6.765371322631836
The interaction time is 14.860830068588257
The total time is 331.1166455745697
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2912.94 ms/it, loss 0.560811
Embedding lookup time: 52.44 ms, MLP time: 511.97 ms, interaction time: 420.03 ms 
Running dlrm: 797.25 ms, backpropagation after dlrm: 2110.65 ms, backpass: 1652.51 ms
The MLP time is 15.871034383773804
The embedding time is 1.6256089210510254
The interaction time is 13.021017789840698
The total time is 332.5561740398407
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2610.78 ms/it, loss 0.560811
Embedding lookup time: 252.08 ms, MLP time: 469.07 ms, interaction time: 453.32 ms 
Running dlrm: 969.66 ms, backpropagation after dlrm: 1636.40 ms, backpass: 1210.92 ms
The MLP time is 14.54125428199768
The embedding time is 7.814548015594482
The interaction time is 14.053064346313477
The total time is 313.2640678882599
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2933.84 ms/it, loss 0.560811
Embedding lookup time: 53.04 ms, MLP time: 498.13 ms, interaction time: 394.49 ms 
Running dlrm: 786.29 ms, backpropagation after dlrm: 2142.24 ms, backpass: 1453.56 ms
The MLP time is 15.442089319229126
The embedding time is 1.6442887783050537
The interaction time is 12.229095935821533
The total time is 531.201681137085
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 3190.48 ms/it, loss 0.560811
Embedding lookup time: 248.84 ms, MLP time: 536.29 ms, interaction time: 482.92 ms 
Running dlrm: 1063.01 ms, backpropagation after dlrm: 2122.59 ms, backpass: 1446.70 ms
The MLP time is 16.624974966049194
The embedding time is 7.71392035484314
The interaction time is 14.970499038696289
The total time is 542.3227798938751
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2992.75 ms/it, loss 0.560811
Embedding lookup time: 52.42 ms, MLP time: 493.09 ms, interaction time: 387.93 ms 
Running dlrm: 777.85 ms, backpropagation after dlrm: 2209.59 ms, backpass: 1493.91 ms
The MLP time is 15.285820245742798
The embedding time is 1.6251449584960938
The interaction time is 12.025798320770264
The total time is 543.7776684761047
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2576.33 ms/it, loss 0.560811
Embedding lookup time: 227.00 ms, MLP time: 458.59 ms, interaction time: 455.90 ms 
Running dlrm: 935.92 ms, backpropagation after dlrm: 1634.87 ms, backpass: 1203.69 ms
The MLP time is 14.216407299041748
The embedding time is 7.0369157791137695
The interaction time is 14.132949829101562
The total time is 307.352899312973
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 3023.66 ms/it, loss 0.560811
Embedding lookup time: 55.70 ms, MLP time: 493.37 ms, interaction time: 388.27 ms 
Running dlrm: 782.21 ms, backpropagation after dlrm: 2236.28 ms, backpass: 1509.16 ms
The MLP time is 15.29434871673584
The embedding time is 1.7267003059387207
The interaction time is 12.036340475082397
The total time is 548.9128000736237
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2721.35 ms/it, loss 0.560811
Embedding lookup time: 248.24 ms, MLP time: 483.02 ms, interaction time: 442.17 ms 
Running dlrm: 975.33 ms, backpropagation after dlrm: 1741.36 ms, backpass: 1262.15 ms
The MLP time is 14.973690271377563
The embedding time is 7.695345401763916
The interaction time is 13.70736575126648
The total time is 346.7864918708801
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2431.29 ms/it, loss 0.560811
Embedding lookup time: 58.29 ms, MLP time: 452.58 ms, interaction time: 355.93 ms 
Running dlrm: 711.97 ms, backpropagation after dlrm: 1713.53 ms, backpass: 1266.72 ms
The MLP time is 14.030123949050903
The embedding time is 1.806900978088379
The interaction time is 11.033972978591919
The total time is 326.43610429763794
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 8192/30697 of epoch 0, 44.09 ms/it, loss 0.486029
Embedding lookup time: 28.79 ms, MLP time: 2.25 ms, interaction time: 3.22 ms 
Running dlrm: 33.16 ms, backpropagation after dlrm: 10.51 ms, backpass: 8.66 ms
Finished training it 16384/30697 of epoch 0, 43.44 ms/it, loss 0.464877
Embedding lookup time: 28.49 ms, MLP time: 2.21 ms, interaction time: 3.16 ms 
Running dlrm: 32.79 ms, backpropagation after dlrm: 10.25 ms, backpass: 8.46 ms
Finished training it 24576/30697 of epoch 0, 48.19 ms/it, loss 0.459999
Embedding lookup time: 31.78 ms, MLP time: 2.34 ms, interaction time: 3.49 ms 
Running dlrm: 36.40 ms, backpropagation after dlrm: 11.35 ms, backpass: 9.27 ms
Finished training it 30697/30697 of epoch 0, 46.20 ms/it, loss 0.457078
Embedding lookup time: 30.26 ms, MLP time: 2.28 ms, interaction time: 3.38 ms 
Running dlrm: 34.76 ms, backpropagation after dlrm: 11.01 ms, backpass: 9.03 ms
The MLP time is 69.63460993766785
The embedding time is 914.8657639026642
The interaction time is 101.63026118278503
The total time is 1649.2888650894165
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 25.17 ms/it, loss 0.486029
Embedding lookup time: 4.63 ms, MLP time: 2.56 ms, interaction time: 3.88 ms 
Running dlrm: 9.49 ms, backpropagation after dlrm: 15.18 ms, backpass: 12.17 ms
Finished training it 16384/30697 of epoch 0, 25.08 ms/it, loss 0.464877
Embedding lookup time: 4.61 ms, MLP time: 2.54 ms, interaction time: 3.87 ms 
Running dlrm: 9.45 ms, backpropagation after dlrm: 15.13 ms, backpass: 12.13 ms
Finished training it 24576/30697 of epoch 0, 25.07 ms/it, loss 0.459999
Embedding lookup time: 4.61 ms, MLP time: 2.54 ms, interaction time: 3.87 ms 
Running dlrm: 9.45 ms, backpropagation after dlrm: 15.13 ms, backpass: 12.13 ms
Finished training it 30697/30697 of epoch 0, 25.12 ms/it, loss 0.457078
Embedding lookup time: 4.63 ms, MLP time: 2.54 ms, interaction time: 3.88 ms 
Running dlrm: 9.48 ms, backpropagation after dlrm: 15.14 ms, backpass: 12.13 ms
The MLP time is 78.1560206413269
The embedding time is 141.7538342475891
The interaction time is 119.02200269699097
The total time is 1224.4908096790314
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 10
Finished training it 8192/30697 of epoch 0, 69.36 ms/it, loss 0.485887
Embedding lookup time: 51.23 ms, MLP time: 2.40 ms, interaction time: 3.42 ms 
Running dlrm: 55.90 ms, backpropagation after dlrm: 13.01 ms, backpass: 10.94 ms
Finished training it 16384/30697 of epoch 0, 68.53 ms/it, loss 0.464886
Embedding lookup time: 50.83 ms, MLP time: 2.32 ms, interaction time: 3.35 ms 
Running dlrm: 55.39 ms, backpropagation after dlrm: 12.69 ms, backpass: 10.68 ms
Finished training it 24576/30697 of epoch 0, 67.19 ms/it, loss 0.460119
Embedding lookup time: 49.70 ms, MLP time: 2.33 ms, interaction time: 3.34 ms 
Running dlrm: 54.26 ms, backpropagation after dlrm: 12.48 ms, backpass: 10.48 ms
Finished training it 30697/30697 of epoch 0, 69.49 ms/it, loss 0.457172
Embedding lookup time: 51.74 ms, MLP time: 2.32 ms, interaction time: 3.38 ms 
Running dlrm: 56.32 ms, backpropagation after dlrm: 12.72 ms, backpass: 10.69 ms
The MLP time is 71.92833805084229
The embedding time is 1559.8961884975433
The interaction time is 103.55451822280884
The total time is 2374.6233949661255
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 24.04 ms/it, loss 0.485887
Embedding lookup time: 4.64 ms, MLP time: 2.15 ms, interaction time: 3.71 ms 
Running dlrm: 9.03 ms, backpropagation after dlrm: 14.50 ms, backpass: 11.49 ms
Finished training it 16384/30697 of epoch 0, 23.88 ms/it, loss 0.464886
Embedding lookup time: 4.61 ms, MLP time: 2.13 ms, interaction time: 3.69 ms 
Running dlrm: 8.95 ms, backpropagation after dlrm: 14.41 ms, backpass: 11.42 ms
Finished training it 24576/30697 of epoch 0, 23.80 ms/it, loss 0.460119
Embedding lookup time: 4.59 ms, MLP time: 2.12 ms, interaction time: 3.69 ms 
Running dlrm: 8.92 ms, backpropagation after dlrm: 14.37 ms, backpass: 11.39 ms
Finished training it 30697/30697 of epoch 0, 23.93 ms/it, loss 0.457172
Embedding lookup time: 4.62 ms, MLP time: 2.13 ms, interaction time: 3.70 ms 
Running dlrm: 8.98 ms, backpropagation after dlrm: 14.45 ms, backpass: 11.45 ms
The MLP time is 65.56165504455566
The embedding time is 141.5988585948944
The interaction time is 113.54869651794434
The total time is 1189.214804649353
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 15
Finished training it 8192/30697 of epoch 0, 40.50 ms/it, loss 0.485887
Embedding lookup time: 22.92 ms, MLP time: 2.28 ms, interaction time: 3.66 ms 
Running dlrm: 27.63 ms, backpropagation after dlrm: 12.38 ms, backpass: 10.63 ms
Finished training it 16384/30697 of epoch 0, 39.48 ms/it, loss 0.464886
Embedding lookup time: 22.31 ms, MLP time: 2.25 ms, interaction time: 3.59 ms 
Running dlrm: 26.96 ms, backpropagation after dlrm: 12.05 ms, backpass: 10.37 ms
Finished training it 24576/30697 of epoch 0, 39.09 ms/it, loss 0.460119
Embedding lookup time: 21.99 ms, MLP time: 2.24 ms, interaction time: 3.54 ms 
Running dlrm: 26.60 ms, backpropagation after dlrm: 12.02 ms, backpass: 10.37 ms
Finished training it 30697/30697 of epoch 0, 40.31 ms/it, loss 0.457172
Embedding lookup time: 23.03 ms, MLP time: 2.25 ms, interaction time: 3.62 ms 
Running dlrm: 27.69 ms, backpropagation after dlrm: 12.14 ms, backpass: 10.46 ms
The MLP time is 69.16739845275879
The embedding time is 691.593427658081
The interaction time is 110.53350853919983
The total time is 1383.8986673355103
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 19.18 ms/it, loss 0.485887
Embedding lookup time: 3.26 ms, MLP time: 2.17 ms, interaction time: 2.32 ms 
Running dlrm: 6.98 ms, backpropagation after dlrm: 11.66 ms, backpass: 9.38 ms
Finished training it 16384/30697 of epoch 0, 19.04 ms/it, loss 0.464886
Embedding lookup time: 3.24 ms, MLP time: 2.15 ms, interaction time: 2.31 ms 
Running dlrm: 6.94 ms, backpropagation after dlrm: 11.57 ms, backpass: 9.30 ms
Finished training it 24576/30697 of epoch 0, 19.18 ms/it, loss 0.460119
Embedding lookup time: 3.29 ms, MLP time: 2.17 ms, interaction time: 2.34 ms 
Running dlrm: 7.02 ms, backpropagation after dlrm: 11.62 ms, backpass: 9.34 ms
Finished training it 30697/30697 of epoch 0, 19.11 ms/it, loss 0.457172
Embedding lookup time: 3.26 ms, MLP time: 2.17 ms, interaction time: 2.33 ms 
Running dlrm: 6.98 ms, backpropagation after dlrm: 11.60 ms, backpass: 9.32 ms
The MLP time is 66.43263077735901
The embedding time is 100.17951989173889
The interaction time is 71.31341195106506
The total time is 821.1452131271362
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 8192/30697 of epoch 0, 50.49 ms/it, loss 0.485887
Embedding lookup time: 28.24 ms, MLP time: 2.48 ms, interaction time: 6.11 ms 
Running dlrm: 34.47 ms, backpropagation after dlrm: 15.53 ms, backpass: 13.76 ms
Finished training it 16384/30697 of epoch 0, 49.85 ms/it, loss 0.464886
Embedding lookup time: 27.74 ms, MLP time: 2.47 ms, interaction time: 6.09 ms 
Running dlrm: 33.95 ms, backpropagation after dlrm: 15.41 ms, backpass: 13.64 ms
Finished training it 24576/30697 of epoch 0, 49.69 ms/it, loss 0.460119
Embedding lookup time: 27.47 ms, MLP time: 2.42 ms, interaction time: 6.27 ms 
Running dlrm: 33.71 ms, backpropagation after dlrm: 15.49 ms, backpass: 13.74 ms
Finished training it 30697/30697 of epoch 0, 50.38 ms/it, loss 0.457172
Embedding lookup time: 28.14 ms, MLP time: 2.45 ms, interaction time: 6.23 ms 
Running dlrm: 34.40 ms, backpropagation after dlrm: 15.50 ms, backpass: 13.74 ms
The MLP time is 75.25348496437073
The embedding time is 855.8664379119873
The interaction time is 189.47763633728027
The total time is 1698.7639191150665
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 19.16 ms/it, loss 0.485887
Embedding lookup time: 3.23 ms, MLP time: 2.06 ms, interaction time: 2.31 ms 
Running dlrm: 6.89 ms, backpropagation after dlrm: 11.63 ms, backpass: 9.30 ms
Finished training it 16384/30697 of epoch 0, 19.11 ms/it, loss 0.464886
Embedding lookup time: 3.23 ms, MLP time: 2.05 ms, interaction time: 2.31 ms 
Running dlrm: 6.88 ms, backpropagation after dlrm: 11.60 ms, backpass: 9.28 ms
Finished training it 24576/30697 of epoch 0, 19.11 ms/it, loss 0.460119
Embedding lookup time: 3.23 ms, MLP time: 2.05 ms, interaction time: 2.31 ms 
Running dlrm: 6.87 ms, backpropagation after dlrm: 11.61 ms, backpass: 9.29 ms
Finished training it 30697/30697 of epoch 0, 19.09 ms/it, loss 0.457172
Embedding lookup time: 3.21 ms, MLP time: 2.05 ms, interaction time: 2.31 ms 
Running dlrm: 6.86 ms, backpropagation after dlrm: 11.60 ms, backpass: 9.28 ms
The MLP time is 62.96772789955139
The embedding time is 99.07879638671875
The interaction time is 70.89595103263855
The total time is 821.7356870174408
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 8192/30697 of epoch 0, 49.74 ms/it, loss 0.485887
Embedding lookup time: 27.44 ms, MLP time: 2.48 ms, interaction time: 6.16 ms 
Running dlrm: 33.70 ms, backpropagation after dlrm: 15.54 ms, backpass: 13.75 ms
Finished training it 16384/30697 of epoch 0, 50.17 ms/it, loss 0.464886
Embedding lookup time: 27.88 ms, MLP time: 2.48 ms, interaction time: 6.18 ms 
Running dlrm: 34.14 ms, backpropagation after dlrm: 15.55 ms, backpass: 13.77 ms
Finished training it 24576/30697 of epoch 0, 49.86 ms/it, loss 0.460119
Embedding lookup time: 27.65 ms, MLP time: 2.44 ms, interaction time: 6.13 ms 
Running dlrm: 33.85 ms, backpropagation after dlrm: 15.52 ms, backpass: 13.74 ms
Finished training it 30697/30697 of epoch 0, 49.44 ms/it, loss 0.457172
Embedding lookup time: 27.22 ms, MLP time: 2.46 ms, interaction time: 6.30 ms 
Running dlrm: 33.53 ms, backpropagation after dlrm: 15.43 ms, backpass: 13.65 ms
The MLP time is 75.70718312263489
The embedding time is 846.266441822052
The interaction time is 189.7530243396759
The total time is 1688.4595243930817
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 19.11 ms/it, loss 0.485887
Embedding lookup time: 3.24 ms, MLP time: 2.05 ms, interaction time: 2.29 ms 
Running dlrm: 6.89 ms, backpropagation after dlrm: 11.59 ms, backpass: 9.24 ms
Finished training it 16384/30697 of epoch 0, 19.15 ms/it, loss 0.464886
Embedding lookup time: 3.26 ms, MLP time: 2.05 ms, interaction time: 2.31 ms 
Running dlrm: 6.91 ms, backpropagation after dlrm: 11.61 ms, backpass: 9.25 ms
Finished training it 24576/30697 of epoch 0, 19.15 ms/it, loss 0.460119
Embedding lookup time: 3.26 ms, MLP time: 2.05 ms, interaction time: 2.30 ms 
Running dlrm: 6.92 ms, backpropagation after dlrm: 11.60 ms, backpass: 9.24 ms
Finished training it 30697/30697 of epoch 0, 19.11 ms/it, loss 0.457172
Embedding lookup time: 3.25 ms, MLP time: 2.05 ms, interaction time: 2.31 ms 
Running dlrm: 6.90 ms, backpropagation after dlrm: 11.58 ms, backpass: 9.24 ms
The MLP time is 62.950567960739136
The embedding time is 99.80464458465576
The interaction time is 70.66661238670349
The total time is 822.1261081695557
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 8192/30697 of epoch 0, 49.81 ms/it, loss 0.485887
Embedding lookup time: 27.83 ms, MLP time: 2.42 ms, interaction time: 6.12 ms 
Running dlrm: 33.99 ms, backpropagation after dlrm: 15.32 ms, backpass: 13.57 ms
Finished training it 16384/30697 of epoch 0, 49.21 ms/it, loss 0.464886
Embedding lookup time: 27.21 ms, MLP time: 2.46 ms, interaction time: 6.12 ms 
Running dlrm: 33.42 ms, backpropagation after dlrm: 15.30 ms, backpass: 13.54 ms
Finished training it 24576/30697 of epoch 0, 49.22 ms/it, loss 0.460119
Embedding lookup time: 27.16 ms, MLP time: 2.43 ms, interaction time: 6.26 ms 
Running dlrm: 33.41 ms, backpropagation after dlrm: 15.32 ms, backpass: 13.58 ms
Finished training it 30697/30697 of epoch 0, 49.31 ms/it, loss 0.457172
Embedding lookup time: 27.26 ms, MLP time: 2.44 ms, interaction time: 6.11 ms 
Running dlrm: 33.45 ms, backpropagation after dlrm: 15.38 ms, backpass: 13.63 ms
The MLP time is 74.79153990745544
The embedding time is 840.2066776752472
The interaction time is 188.9144847393036
The total time is 1673.4222331047058
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 19.33 ms/it, loss 0.485887
Embedding lookup time: 3.25 ms, MLP time: 2.07 ms, interaction time: 2.33 ms 
Running dlrm: 6.93 ms, backpropagation after dlrm: 11.76 ms, backpass: 9.42 ms
Finished training it 16384/30697 of epoch 0, 19.17 ms/it, loss 0.464886
Embedding lookup time: 3.24 ms, MLP time: 2.05 ms, interaction time: 2.31 ms 
Running dlrm: 6.88 ms, backpropagation after dlrm: 11.66 ms, backpass: 9.35 ms
Finished training it 24576/30697 of epoch 0, 19.24 ms/it, loss 0.460119
Embedding lookup time: 3.26 ms, MLP time: 2.06 ms, interaction time: 2.33 ms 
Running dlrm: 6.93 ms, backpropagation after dlrm: 11.68 ms, backpass: 9.36 ms
Finished training it 30697/30697 of epoch 0, 19.16 ms/it, loss 0.457172
Embedding lookup time: 3.23 ms, MLP time: 2.05 ms, interaction time: 2.31 ms 
Running dlrm: 6.88 ms, backpropagation after dlrm: 11.65 ms, backpass: 9.34 ms
The MLP time is 63.065667152404785
The embedding time is 99.68077397346497
The interaction time is 71.20700430870056
The total time is 824.7050471305847
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 8192/30697 of epoch 0, 49.99 ms/it, loss 0.485887
Embedding lookup time: 27.70 ms, MLP time: 2.50 ms, interaction time: 6.15 ms 
Running dlrm: 33.98 ms, backpropagation after dlrm: 15.52 ms, backpass: 13.74 ms
Finished training it 16384/30697 of epoch 0, 49.29 ms/it, loss 0.464886
Embedding lookup time: 27.16 ms, MLP time: 2.42 ms, interaction time: 6.33 ms 
Running dlrm: 33.44 ms, backpropagation after dlrm: 15.37 ms, backpass: 13.62 ms
Finished training it 24576/30697 of epoch 0, 49.51 ms/it, loss 0.460119
Embedding lookup time: 27.42 ms, MLP time: 2.41 ms, interaction time: 6.26 ms 
Running dlrm: 33.65 ms, backpropagation after dlrm: 15.38 ms, backpass: 13.62 ms
Finished training it 30697/30697 of epoch 0, 50.11 ms/it, loss 0.457172
Embedding lookup time: 27.99 ms, MLP time: 2.39 ms, interaction time: 6.30 ms 
Running dlrm: 34.23 ms, backpropagation after dlrm: 15.40 ms, backpass: 13.64 ms
The MLP time is 74.69407987594604
The embedding time is 845.3666315078735
The interaction time is 192.0651478767395
The total time is 1686.81995844841
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 8192/30697 of epoch 0, 19.15 ms/it, loss 0.485887
Embedding lookup time: 3.27 ms, MLP time: 2.05 ms, interaction time: 2.31 ms 
Running dlrm: 6.93 ms, backpropagation after dlrm: 11.58 ms, backpass: 9.24 ms
Finished training it 16384/30697 of epoch 0, 19.01 ms/it, loss 0.464886
Embedding lookup time: 3.23 ms, MLP time: 2.04 ms, interaction time: 2.30 ms 
Running dlrm: 6.87 ms, backpropagation after dlrm: 11.52 ms, backpass: 9.20 ms
Finished training it 24576/30697 of epoch 0, 19.03 ms/it, loss 0.460119
Embedding lookup time: 3.23 ms, MLP time: 2.04 ms, interaction time: 2.30 ms 
Running dlrm: 6.87 ms, backpropagation after dlrm: 11.53 ms, backpass: 9.20 ms
Finished training it 30697/30697 of epoch 0, 19.06 ms/it, loss 0.457172
Embedding lookup time: 3.25 ms, MLP time: 2.05 ms, interaction time: 2.32 ms 
Running dlrm: 6.90 ms, backpropagation after dlrm: 11.53 ms, backpass: 9.21 ms
The MLP time is 62.762837409973145
The embedding time is 99.6454439163208
The interaction time is 70.80706334114075
The total time is 817.8146412372589
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

below is for real:
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2540.99 ms/it, loss 0.560811
Embedding lookup time: 186.26 ms, MLP time: 462.62 ms, interaction time: 445.80 ms 
Running dlrm: 892.45 ms, backpropagation after dlrm: 1644.00 ms, backpass: 1187.79 ms
The MLP time is 14.34128737449646
The embedding time is 5.77402925491333
The interaction time is 13.819881200790405
The total time is 300.54974484443665
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 2111, in <module>
    run()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 1706, in run
    for j, inputBatch in enumerate(train_ld):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 764, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py", line 336, in collate_wrapper_criteo_offset
    lS_o = [torch.tensor(range(batchSize)) for _ in range(featureCnt)]
  File "/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py", line 336, in <listcomp>
    lS_o = [torch.tensor(range(batchSize)) for _ in range(featureCnt)]
KeyboardInterrupt
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 25
Finished training it 31/31 of epoch 0, 3937.06 ms/it, loss 0.560811
Embedding lookup time: 322.80 ms, MLP time: 588.90 ms, interaction time: 485.82 ms 
Running dlrm: 1193.18 ms, backpropagation after dlrm: 2738.56 ms, backpass: 2128.65 ms
The MLP time is 18.25600528717041
The embedding time is 10.006817817687988
The interaction time is 15.060394763946533
The total time is 581.4170532226562
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2305.88 ms/it, loss 0.560811
Embedding lookup time: 49.19 ms, MLP time: 433.03 ms, interaction time: 336.80 ms 
Running dlrm: 671.30 ms, backpropagation after dlrm: 1629.51 ms, backpass: 1204.17 ms
The MLP time is 13.423947095870972
The embedding time is 1.5247561931610107
The interaction time is 10.440860509872437
The total time is 295.3684482574463
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 32
Finished training it 31/31 of epoch 0, 2540.76 ms/it, loss 0.560811
Embedding lookup time: 265.15 ms, MLP time: 470.47 ms, interaction time: 441.25 ms 
Running dlrm: 979.71 ms, backpropagation after dlrm: 1558.25 ms, backpass: 1266.74 ms
The MLP time is 14.584450721740723
The embedding time is 8.219568490982056
The interaction time is 13.678733825683594
The total time is 300.8841321468353
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
Finished training it 31/31 of epoch 0, 2283.25 ms/it, loss 0.560811
Embedding lookup time: 44.32 ms, MLP time: 431.34 ms, interaction time: 338.17 ms 
Running dlrm: 664.99 ms, backpropagation after dlrm: 1613.01 ms, backpass: 1183.33 ms
The MLP time is 13.371640682220459
The embedding time is 1.3737692832946777
The interaction time is 10.483323097229004
The total time is 288.3532636165619
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 40
Finished training it 31/31 of epoch 0, 3260.95 ms/it, loss 0.560811
Embedding lookup time: 343.98 ms, MLP time: 518.27 ms, interaction time: 719.23 ms 
Running dlrm: 1262.68 ms, backpropagation after dlrm: 1988.10 ms, backpass: 1541.00 ms
The MLP time is 16.066517114639282
The embedding time is 10.663338422775269
The interaction time is 22.29625916481018
The total time is 561.5353889465332
Command used to run the program: dlrm_para_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2248.71 ms/it, loss 0.560811
Embedding lookup time: 53.56 ms, MLP time: 419.28 ms, interaction time: 334.17 ms 
Running dlrm: 660.46 ms, backpropagation after dlrm: 1583.40 ms, backpass: 1169.45 ms
The MLP time is 12.997668504714966
The embedding time is 1.6602556705474854
The interaction time is 10.359319686889648
The total time is 291.7383117675781
Command used to run the program: dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16


below is running with right number of threads===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 40
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 2173, in <module>
    run()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 1768, in run
    for j, inputBatch in enumerate(train_ld):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 764, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py", line 328, in collate_wrapper_criteo_offset
    X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
KeyboardInterrupt

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 2113, in <module>
    
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 1745, in run
    # start_time = time.time()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 212, in dlrm_wrap
    return dlrm(X.to(device), lS_o, lS_i)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 613, in forward
    return self.sequential_forward(dense_x, lS_o, lS_i)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 701, in sequential_forward
    x = self.apply_mlp(dense_x, self.bot_l)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 477, in apply_mlp
    return layers(x)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 31/31 of epoch 0, 5742.33 ms/it, loss 0.560811
Embedding lookup time: 85.46 ms, MLP time: 1152.07 ms, interaction time: 1006.86 ms 
Running dlrm: 1760.72 ms, backpropagation after dlrm: 3977.22 ms, backpass: 3519.57 ms
The MLP time is 35.7142014503479
The embedding time is 2.649264335632324
The interaction time is 31.21254062652588
The total time is 405.4023962020874
Command used to run the program: dlrm_para_pytorch.py --thread-count=5 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 2116, in <module>
    run()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 1711, in run
    for j, inputBatch in enumerate(train_ld):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 764, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py", line 336, in collate_wrapper_criteo_offset
    lS_o = [torch.tensor(range(batchSize)) for _ in range(featureCnt)]
  File "/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py", line 336, in <listcomp>
    lS_o = [torch.tensor(range(batchSize)) for _ in range(featureCnt)]
KeyboardInterrupt
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 2177, in <module>
    run()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 1809, in run
    Z = dlrm_wrap(
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 215, in dlrm_wrap
    return dlrm(X.to(device), lS_o, lS_i)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 673, in forward
    return self.sequential_forward(dense_x, lS_o, lS_i)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 775, in sequential_forward
    ly = self.apply_emb(lS_o, lS_i, self.emb_l, self.v_W_l)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 486, in apply_emb
NameError: name 'thread_count' is not defined

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 2116, in <module>
    run()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 1799, in run
    optimizer.step()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/optim/sgd.py", line 125, in step
    sgd(
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/optim/sgd.py", line 300, in sgd
    func(
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/optim/sgd.py", line 353, in _single_tensor_sgd
    param.add_(grad, alpha=-lr)
KeyboardInterrupt
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 1502
    global thread_count = args.thread_count
                        ^
SyntaxError: invalid syntax

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 2116, in <module>
    run()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 1790, in run
    E.backward()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 2177, in <module>
    run()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 1809, in run
    Z = dlrm_wrap(
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 215, in dlrm_wrap
    return dlrm(X.to(device), lS_o, lS_i)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 673, in forward
    return self.sequential_forward(dense_x, lS_o, lS_i)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 775, in sequential_forward
    ly = self.apply_emb(lS_o, lS_i, self.emb_l, self.v_W_l)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 486, in apply_emb
NameError: name 'thread_count' is not defined

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 31/31 of epoch 0, 6440.31 ms/it, loss 0.560811
Embedding lookup time: 88.30 ms, MLP time: 1249.66 ms, interaction time: 1088.39 ms 
Running dlrm: 1920.11 ms, backpropagation after dlrm: 4514.80 ms, backpass: 3700.57 ms
The MLP time is 38.73936080932617
The embedding time is 2.7372918128967285
The interaction time is 33.740159034729004
The total time is 654.9932277202606
Command used to run the program: dlrm_s_pytorch.py --thread-count=5 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 55, in <module>
    import torch.jit
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/__init__.py", line 405, in <module>
    from torch._C import *  # noqa: F403
  File "<frozen importlib._bootstrap>", line 216, in _lock_unlock_module
KeyboardInterrupt
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 2179, in <module>
    run()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 1508, in run
    dlrm = DLRM_Net(
TypeError: DLRM_Net.__init__() got an unexpected keyword argument 'thread_count'

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 31/31 of epoch 0, 6460.20 ms/it, loss 0.560811
Embedding lookup time: 87.66 ms, MLP time: 1252.45 ms, interaction time: 1092.62 ms 
Running dlrm: 1924.34 ms, backpropagation after dlrm: 4531.00 ms, backpass: 3712.58 ms
The MLP time is 38.82594013214111
The embedding time is 2.717377185821533
The interaction time is 33.871354818344116
The total time is 657.6626369953156
Command used to run the program: dlrm_s_pytorch.py --thread-count=5 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 2179, in <module>
    run()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 1320, in run
    train_data, train_ld, test_data, test_ld = dp.make_criteo_data_and_loaders(args)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py", line 520, in make_criteo_data_and_loaders
    train_data = CriteoDataset(
  File "/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py", line 205, in __init__
    X_cat = data["X_cat"]  # categorical feature
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py", line 254, in __getitem__
    return format.read_array(bytes,
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/numpy/lib/format.py", line 851, in read_array
    data = _read_bytes(fp, read_size, "array data")
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/numpy/lib/format.py", line 986, in _read_bytes
    r = fp.read(size - len(data))
  File "/usr/lib/python3.10/zipfile.py", line 930, in read
    data = self._read1(n)
  File "/usr/lib/python3.10/zipfile.py", line 1020, in _read1
    self._update_crc(data)
  File "/usr/lib/python3.10/zipfile.py", line 945, in _update_crc
    self._running_crc = crc32(newdata, self._running_crc)
KeyboardInterrupt
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 2179, in <module>
    run()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_para_pytorch.py", line 1508, in run
    dlrm = DLRM_Net(
TypeError: DLRM_Net.__init__() got an unexpected keyword argument 'thread_count'

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Traceback (most recent call last):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 2116, in <module>
    run()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_s_pytorch.py", line 1711, in run
    for j, inputBatch in enumerate(train_ld):
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 764, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/users/mt1370/expr/dlrm_minrui/dlrm_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py", line 329, in collate_wrapper_criteo_offset
    X_cat = torch.tensor(transposed_data[1], dtype=torch.long)
KeyboardInterrupt
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 31/31 of epoch 0, 4464.41 ms/it, loss 0.560811
Embedding lookup time: 100.94 ms, MLP time: 900.96 ms, interaction time: 697.31 ms 
Running dlrm: 1369.46 ms, backpropagation after dlrm: 3092.51 ms, backpass: 2678.45 ms
The MLP time is 27.9296395778656
The embedding time is 3.1290903091430664
The interaction time is 21.61667823791504
The total time is 360.5439546108246
Command used to run the program: dlrm_para_pytorch.py --thread-count=5 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 31/31 of epoch 0, 6454.55 ms/it, loss 0.560811
Embedding lookup time: 83.80 ms, MLP time: 1249.80 ms, interaction time: 1091.45 ms 
Running dlrm: 1917.28 ms, backpropagation after dlrm: 4531.09 ms, backpass: 3714.02 ms
The MLP time is 38.743929624557495
The embedding time is 2.597672700881958
The interaction time is 33.834851026535034
The total time is 653.8230111598969
Command used to run the program: dlrm_s_pytorch.py --thread-count=5 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 10
Finished training it 31/31 of epoch 0, 3266.92 ms/it, loss 0.560811
Embedding lookup time: 92.62 ms, MLP time: 657.04 ms, interaction time: 557.05 ms 
Running dlrm: 1048.55 ms, backpropagation after dlrm: 2213.86 ms, backpass: 1778.73 ms
The MLP time is 20.368335962295532
The embedding time is 2.8711726665496826
The interaction time is 17.26856517791748
The total time is 324.1863524913788
Command used to run the program: dlrm_para_pytorch.py --thread-count=10 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 10
Finished training it 31/31 of epoch 0, 3937.81 ms/it, loss 0.560811
Embedding lookup time: 66.24 ms, MLP time: 709.41 ms, interaction time: 589.85 ms 
Running dlrm: 1108.36 ms, backpropagation after dlrm: 2824.73 ms, backpass: 2052.80 ms
The MLP time is 21.99163556098938
The embedding time is 2.0533740520477295
The interaction time is 18.285348653793335
The total time is 574.1358096599579
Command used to run the program: dlrm_s_pytorch.py --thread-count=10 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2432.23 ms/it, loss 0.560811
Embedding lookup time: 114.94 ms, MLP time: 448.70 ms, interaction time: 376.31 ms 
Running dlrm: 773.41 ms, backpropagation after dlrm: 1654.26 ms, backpass: 1199.10 ms
The MLP time is 13.909725666046143
The embedding time is 3.5630409717559814
The interaction time is 11.665497541427612
The total time is 297.5954792499542
Command used to run the program: dlrm_para_pytorch.py --thread-count=20 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 31/31 of epoch 0, 2965.57 ms/it, loss 0.560811
Embedding lookup time: 58.57 ms, MLP time: 500.19 ms, interaction time: 393.81 ms 
Running dlrm: 794.08 ms, backpropagation after dlrm: 2166.12 ms, backpass: 1468.54 ms
The MLP time is 15.505872011184692
The embedding time is 1.815694808959961
The interaction time is 12.208083629608154
The total time is 534.7794389724731
Command used to run the program: dlrm_s_pytorch.py --thread-count=20 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 25
Finished training it 31/31 of epoch 0, 3118.44 ms/it, loss 0.560811
Embedding lookup time: 97.62 ms, MLP time: 553.00 ms, interaction time: 415.40 ms 
Running dlrm: 881.45 ms, backpropagation after dlrm: 2232.47 ms, backpass: 1859.21 ms
The MLP time is 17.14294958114624
The embedding time is 3.026157855987549
The interaction time is 12.877372026443481
The total time is 322.97711968421936
Command used to run the program: dlrm_para_pytorch.py --thread-count=25 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 25
Finished training it 31/31 of epoch 0, 3761.52 ms/it, loss 0.560811
Embedding lookup time: 61.95 ms, MLP time: 606.47 ms, interaction time: 447.84 ms 
Running dlrm: 931.13 ms, backpropagation after dlrm: 2824.38 ms, backpass: 2250.88 ms
The MLP time is 18.8005211353302
The embedding time is 1.920480489730835
The interaction time is 13.882981538772583
The total time is 565.8226668834686
Command used to run the program: dlrm_s_pytorch.py --thread-count=25 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 32
Finished training it 31/31 of epoch 0, 2420.26 ms/it, loss 0.560811
Embedding lookup time: 86.41 ms, MLP time: 480.21 ms, interaction time: 371.35 ms 
Running dlrm: 773.64 ms, backpropagation after dlrm: 1642.57 ms, backpass: 1278.50 ms
The MLP time is 14.886455774307251
The embedding time is 2.6788461208343506
The interaction time is 11.511924266815186
The total time is 295.54151129722595
Command used to run the program: dlrm_para_pytorch.py --thread-count=32 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 32
Finished training it 31/31 of epoch 0, 2970.42 ms/it, loss 0.560811
Embedding lookup time: 53.15 ms, MLP time: 519.64 ms, interaction time: 393.80 ms 
Running dlrm: 808.46 ms, backpropagation after dlrm: 2155.93 ms, backpass: 1533.54 ms
The MLP time is 16.108941316604614
The embedding time is 1.6477015018463135
The interaction time is 12.207674980163574
The total time is 538.9575009346008
Command used to run the program: dlrm_s_pytorch.py --thread-count=32 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 40
Finished training it 31/31 of epoch 0, 2857.63 ms/it, loss 0.560811
Embedding lookup time: 130.65 ms, MLP time: 480.96 ms, interaction time: 533.03 ms 
Running dlrm: 916.88 ms, backpropagation after dlrm: 1935.67 ms, backpass: 1496.76 ms
The MLP time is 14.909756898880005
The embedding time is 4.050081968307495
The interaction time is 16.524040699005127
The total time is 511.6907114982605
Command used to run the program: dlrm_para_pytorch.py --thread-count=40 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 40
Finished training it 31/31 of epoch 0, 2681.62 ms/it, loss 0.560811
Embedding lookup time: 59.41 ms, MLP time: 492.65 ms, interaction time: 369.46 ms 
Running dlrm: 776.76 ms, backpropagation after dlrm: 1900.31 ms, backpass: 1473.79 ms
The MLP time is 15.272043943405151
The embedding time is 1.8416109085083008
The interaction time is 11.453400135040283
The total time is 529.8677880764008
Command used to run the program: dlrm_s_pytorch.py --thread-count=40 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128000 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 3070/3070 of epoch 0, 57.52 ms/it, loss 0.492458
Embedding lookup time: 9.45 ms, MLP time: 8.39 ms, interaction time: 7.44 ms 
Running dlrm: 22.09 ms, backpropagation after dlrm: 34.86 ms, backpass: 27.93 ms
The MLP time is 25.747657299041748
The embedding time is 29.016918659210205
The interaction time is 22.84001851081848
The total time is 380.9933354854584
Command used to run the program: dlrm_para_pytorch.py --thread-count=5 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=1280 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 3070/3070 of epoch 0, 68.85 ms/it, loss 0.492553
Embedding lookup time: 5.70 ms, MLP time: 9.70 ms, interaction time: 10.48 ms 
Running dlrm: 21.11 ms, backpropagation after dlrm: 47.00 ms, backpass: 35.51 ms
The MLP time is 29.774059295654297
The embedding time is 17.498379945755005
The interaction time is 32.179462909698486
The total time is 634.1706697940826
Command used to run the program: dlrm_s_pytorch.py --thread-count=5 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=1280 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 10
Finished training it 3070/3070 of epoch 0, 57.49 ms/it, loss 0.492492
Embedding lookup time: 14.47 ms, MLP time: 6.02 ms, interaction time: 7.39 ms 
Running dlrm: 24.76 ms, backpropagation after dlrm: 32.07 ms, backpass: 23.17 ms
The MLP time is 18.475390434265137
The embedding time is 44.43809533119202
The interaction time is 22.69690775871277
The total time is 516.2480881214142
Command used to run the program: dlrm_para_pytorch.py --thread-count=10 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=1280 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 10
Finished training it 3070/3070 of epoch 0, 40.26 ms/it, loss 0.492492
Embedding lookup time: 3.55 ms, MLP time: 5.81 ms, interaction time: 6.54 ms 
Running dlrm: 12.97 ms, backpropagation after dlrm: 26.73 ms, backpass: 20.26 ms
The MLP time is 17.847982168197632
The embedding time is 10.891221046447754
The interaction time is 20.075544595718384
The total time is 330.00977635383606
Command used to run the program: dlrm_s_pytorch.py --thread-count=10 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=1280 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 3070/3070 of epoch 0, 56.92 ms/it, loss 0.492565
Embedding lookup time: 22.64 ms, MLP time: 5.23 ms, interaction time: 5.95 ms 
Running dlrm: 31.37 ms, backpropagation after dlrm: 24.99 ms, backpass: 18.62 ms
The MLP time is 16.04702067375183
The embedding time is 69.51595091819763
The interaction time is 18.265198230743408
The total time is 387.26484513282776
Command used to run the program: dlrm_para_pytorch.py --thread-count=20 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=1280 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 3070/3070 of epoch 0, 36.68 ms/it, loss 0.492565
Embedding lookup time: 3.65 ms, MLP time: 4.78 ms, interaction time: 5.87 ms 
Running dlrm: 11.71 ms, backpropagation after dlrm: 24.44 ms, backpass: 17.31 ms
The MLP time is 14.664590835571289
The embedding time is 11.210472106933594
The interaction time is 18.01811981201172
The total time is 344.68297028541565
Command used to run the program: dlrm_s_pytorch.py --thread-count=20 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=1280 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 25
Finished training it 3070/3070 of epoch 0, 74.81 ms/it, loss 0.492411
Embedding lookup time: 37.69 ms, MLP time: 5.72 ms, interaction time: 7.05 ms 
Running dlrm: 47.49 ms, backpropagation after dlrm: 26.72 ms, backpass: 20.06 ms
The MLP time is 17.561466693878174
The embedding time is 115.69872212409973
The interaction time is 21.630568265914917
The total time is 468.0148811340332
Command used to run the program: dlrm_para_pytorch.py --thread-count=25 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=1280 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 25
Finished training it 3070/3070 of epoch 0, 49.76 ms/it, loss 0.492411
Embedding lookup time: 5.27 ms, MLP time: 6.21 ms, interaction time: 7.24 ms 
Running dlrm: 15.54 ms, backpropagation after dlrm: 33.53 ms, backpass: 23.79 ms
The MLP time is 19.061234951019287
The embedding time is 16.188225746154785
The interaction time is 22.224955797195435
The total time is 561.4133780002594
Command used to run the program: dlrm_s_pytorch.py --thread-count=25 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=1280 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 32
Finished training it 3070/3070 of epoch 0, 86.02 ms/it, loss 0.492446
Embedding lookup time: 40.79 ms, MLP time: 8.43 ms, interaction time: 11.05 ms 
Running dlrm: 55.27 ms, backpropagation after dlrm: 30.22 ms, backpass: 22.50 ms
The MLP time is 25.88483166694641
The embedding time is 125.23223328590393
The interaction time is 33.92789387702942
The total time is 413.28453540802
Command used to run the program: dlrm_para_pytorch.py --thread-count=32 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=1280 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 32
Finished training it 3070/3070 of epoch 0, 37.72 ms/it, loss 0.492446
Embedding lookup time: 4.00 ms, MLP time: 5.22 ms, interaction time: 5.63 ms 
Running dlrm: 12.42 ms, backpropagation after dlrm: 24.71 ms, backpass: 17.47 ms
The MLP time is 16.01380491256714
The embedding time is 12.286821365356445
The interaction time is 17.293712854385376
The total time is 387.94186210632324
Command used to run the program: dlrm_s_pytorch.py --thread-count=32 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=1280 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 40
Finished training it 3070/3070 of epoch 0, 181.62 ms/it, loss 0.492575
Embedding lookup time: 42.12 ms, MLP time: 32.14 ms, interaction time: 30.39 ms 
Running dlrm: 89.96 ms, backpropagation after dlrm: 91.13 ms, backpass: 62.78 ms
The MLP time is 98.67099022865295
The embedding time is 129.3149516582489
The interaction time is 93.30930256843567
The total time is 700.507161617279
Command used to run the program: dlrm_para_pytorch.py --thread-count=40 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=1280 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 40
Finished training it 3070/3070 of epoch 0, 51.08 ms/it, loss 0.492575
Embedding lookup time: 7.48 ms, MLP time: 11.20 ms, interaction time: 5.70 ms 
Running dlrm: 21.91 ms, backpropagation after dlrm: 28.59 ms, backpass: 19.80 ms
The MLP time is 34.38752841949463
The embedding time is 22.955034494400024
The interaction time is 17.50009250640869
The total time is 405.85960054397583
Command used to run the program: dlrm_s_pytorch.py --thread-count=40 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=1280 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 8192/30697 of epoch 0, 23.52 ms/it, loss 0.486029
Embedding lookup time: 8.90 ms, MLP time: 2.12 ms, interaction time: 2.82 ms 
Running dlrm: 12.83 ms, backpropagation after dlrm: 10.27 ms, backpass: 8.38 ms
Finished training it 16384/30697 of epoch 0, 23.40 ms/it, loss 0.464877
Embedding lookup time: 8.88 ms, MLP time: 2.07 ms, interaction time: 2.78 ms 
Running dlrm: 12.76 ms, backpropagation after dlrm: 10.23 ms, backpass: 8.37 ms
Finished training it 24576/30697 of epoch 0, 23.28 ms/it, loss 0.459999
Embedding lookup time: 8.57 ms, MLP time: 2.10 ms, interaction time: 2.80 ms 
Running dlrm: 12.48 ms, backpropagation after dlrm: 10.39 ms, backpass: 8.50 ms
Finished training it 30697/30697 of epoch 0, 22.90 ms/it, loss 0.457078
Embedding lookup time: 8.60 ms, MLP time: 2.04 ms, interaction time: 2.76 ms 
Running dlrm: 12.43 ms, backpropagation after dlrm: 10.06 ms, backpass: 8.22 ms
The MLP time is 64.05773615837097
The embedding time is 268.54203820228577
The interaction time is 85.71151518821716
The total time is 943.3386392593384
Command used to run the program: dlrm_para_pytorch.py --thread-count=5 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 5
Finished training it 8192/30697 of epoch 0, 25.19 ms/it, loss 0.486029
Embedding lookup time: 4.59 ms, MLP time: 2.55 ms, interaction time: 3.91 ms 
Running dlrm: 9.46 ms, backpropagation after dlrm: 15.23 ms, backpass: 12.23 ms
Finished training it 16384/30697 of epoch 0, 25.12 ms/it, loss 0.464877
Embedding lookup time: 4.57 ms, MLP time: 2.54 ms, interaction time: 3.89 ms 
Running dlrm: 9.43 ms, backpropagation after dlrm: 15.20 ms, backpass: 12.20 ms
Finished training it 24576/30697 of epoch 0, 25.07 ms/it, loss 0.459999
Embedding lookup time: 4.56 ms, MLP time: 2.53 ms, interaction time: 3.88 ms 
Running dlrm: 9.40 ms, backpropagation after dlrm: 15.18 ms, backpass: 12.19 ms
Finished training it 30697/30697 of epoch 0, 25.07 ms/it, loss 0.457078
Embedding lookup time: 4.56 ms, MLP time: 2.54 ms, interaction time: 3.88 ms 
Running dlrm: 9.41 ms, backpropagation after dlrm: 15.17 ms, backpass: 12.19 ms
The MLP time is 77.98972129821777
The embedding time is 140.28055882453918
The interaction time is 119.4414575099945
The total time is 1224.264891386032
Command used to run the program: dlrm_s_pytorch.py --thread-count=5 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 10
Finished training it 8192/30697 of epoch 0, 22.92 ms/it, loss 0.485887
Embedding lookup time: 9.70 ms, MLP time: 1.70 ms, interaction time: 2.59 ms 
Running dlrm: 13.10 ms, backpropagation after dlrm: 9.41 ms, backpass: 7.61 ms
Finished training it 16384/30697 of epoch 0, 22.95 ms/it, loss 0.464886
Embedding lookup time: 9.73 ms, MLP time: 1.70 ms, interaction time: 2.60 ms 
Running dlrm: 13.14 ms, backpropagation after dlrm: 9.41 ms, backpass: 7.61 ms
Finished training it 24576/30697 of epoch 0, 23.12 ms/it, loss 0.460119
Embedding lookup time: 9.82 ms, MLP time: 1.70 ms, interaction time: 2.60 ms 
Running dlrm: 13.23 ms, backpropagation after dlrm: 9.48 ms, backpass: 7.66 ms
Finished training it 30697/30697 of epoch 0, 22.92 ms/it, loss 0.457172
Embedding lookup time: 9.88 ms, MLP time: 1.67 ms, interaction time: 2.58 ms 
Running dlrm: 13.25 ms, backpropagation after dlrm: 9.27 ms, backpass: 7.49 ms
The MLP time is 52.025384187698364
The embedding time is 300.1213471889496
The interaction time is 79.60718441009521
The total time is 929.9687294960022
Command used to run the program: dlrm_para_pytorch.py --thread-count=10 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 10
Finished training it 8192/30697 of epoch 0, 23.94 ms/it, loss 0.485887
Embedding lookup time: 4.68 ms, MLP time: 2.14 ms, interaction time: 3.70 ms 
Running dlrm: 9.04 ms, backpropagation after dlrm: 14.39 ms, backpass: 11.40 ms
Finished training it 16384/30697 of epoch 0, 23.92 ms/it, loss 0.464886
Embedding lookup time: 4.66 ms, MLP time: 2.14 ms, interaction time: 3.70 ms 
Running dlrm: 9.02 ms, backpropagation after dlrm: 14.39 ms, backpass: 11.41 ms
Finished training it 24576/30697 of epoch 0, 23.86 ms/it, loss 0.460119
Embedding lookup time: 4.67 ms, MLP time: 2.13 ms, interaction time: 3.69 ms 
Running dlrm: 9.02 ms, backpropagation after dlrm: 14.34 ms, backpass: 11.36 ms
Finished training it 30697/30697 of epoch 0, 23.82 ms/it, loss 0.457172
Embedding lookup time: 4.65 ms, MLP time: 2.13 ms, interaction time: 3.69 ms 
Running dlrm: 8.99 ms, backpropagation after dlrm: 14.32 ms, backpass: 11.35 ms
The MLP time is 65.52513217926025
The embedding time is 143.22327160835266
The interaction time is 113.34746479988098
The total time is 1187.4023396968842
Command used to run the program: dlrm_s_pytorch.py --thread-count=10 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 8192/30697 of epoch 0, 36.49 ms/it, loss 0.485913
Embedding lookup time: 23.66 ms, MLP time: 1.67 ms, interaction time: 2.84 ms 
Running dlrm: 27.16 ms, backpropagation after dlrm: 8.92 ms, backpass: 7.35 ms
Finished training it 16384/30697 of epoch 0, 36.49 ms/it, loss 0.464865
Embedding lookup time: 23.81 ms, MLP time: 1.65 ms, interaction time: 2.80 ms 
Running dlrm: 27.26 ms, backpropagation after dlrm: 8.82 ms, backpass: 7.30 ms
Finished training it 24576/30697 of epoch 0, 36.79 ms/it, loss 0.460121
Embedding lookup time: 24.10 ms, MLP time: 1.65 ms, interaction time: 2.79 ms 
Running dlrm: 27.56 ms, backpropagation after dlrm: 8.82 ms, backpass: 7.28 ms
Finished training it 30697/30697 of epoch 0, 35.80 ms/it, loss 0.457142
Embedding lookup time: 23.33 ms, MLP time: 1.62 ms, interaction time: 2.77 ms 
Running dlrm: 26.74 ms, backpropagation after dlrm: 8.65 ms, backpass: 7.17 ms
The MLP time is 50.60624980926514
The embedding time is 729.1178302764893
The interaction time is 86.06482434272766
The total time is 1253.352745771408
Command used to run the program: dlrm_para_pytorch.py --thread-count=20 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 20
Finished training it 8192/30697 of epoch 0, 18.36 ms/it, loss 0.485913
Embedding lookup time: 3.18 ms, MLP time: 2.01 ms, interaction time: 2.31 ms 
Running dlrm: 6.65 ms, backpropagation after dlrm: 11.30 ms, backpass: 9.17 ms
Finished training it 16384/30697 of epoch 0, 18.37 ms/it, loss 0.464865
Embedding lookup time: 3.19 ms, MLP time: 1.99 ms, interaction time: 2.30 ms 
Running dlrm: 6.64 ms, backpropagation after dlrm: 11.32 ms, backpass: 9.18 ms
Finished training it 24576/30697 of epoch 0, 18.34 ms/it, loss 0.460121
Embedding lookup time: 3.19 ms, MLP time: 2.00 ms, interaction time: 2.31 ms 
Running dlrm: 6.65 ms, backpropagation after dlrm: 11.28 ms, backpass: 9.15 ms
Finished training it 30697/30697 of epoch 0, 18.29 ms/it, loss 0.457142
Embedding lookup time: 3.17 ms, MLP time: 1.99 ms, interaction time: 2.29 ms 
Running dlrm: 6.62 ms, backpropagation after dlrm: 11.26 ms, backpass: 9.13 ms
The MLP time is 61.32064080238342
The embedding time is 97.69446778297424
The interaction time is 70.67228674888611
The total time is 796.8768169879913
Command used to run the program: dlrm_s_pytorch.py --thread-count=20 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 25
Finished training it 8192/30697 of epoch 0, 63.10 ms/it, loss 0.485913
Embedding lookup time: 36.93 ms, MLP time: 1.66 ms, interaction time: 4.09 ms 
Running dlrm: 41.06 ms, backpropagation after dlrm: 21.62 ms, backpass: 19.84 ms
Finished training it 16384/30697 of epoch 0, 63.99 ms/it, loss 0.464865
Embedding lookup time: 37.52 ms, MLP time: 1.67 ms, interaction time: 4.07 ms 
Running dlrm: 41.65 ms, backpropagation after dlrm: 21.93 ms, backpass: 20.08 ms
Finished training it 24576/30697 of epoch 0, 64.29 ms/it, loss 0.460121
Embedding lookup time: 37.44 ms, MLP time: 1.65 ms, interaction time: 4.29 ms 
Running dlrm: 41.66 ms, backpropagation after dlrm: 22.21 ms, backpass: 20.44 ms
Finished training it 30697/30697 of epoch 0, 64.02 ms/it, loss 0.457142
Embedding lookup time: 37.88 ms, MLP time: 1.66 ms, interaction time: 4.21 ms 
Running dlrm: 42.08 ms, backpropagation after dlrm: 21.53 ms, backpass: 19.70 ms
The MLP time is 50.96709156036377
The embedding time is 1148.497992515564
The interaction time is 127.76291036605835
The total time is 2102.5765573978424
Command used to run the program: dlrm_para_pytorch.py --thread-count=25 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 25
Finished training it 8192/30697 of epoch 0, 37.53 ms/it, loss 0.485913
Embedding lookup time: 3.32 ms, MLP time: 1.91 ms, interaction time: 2.29 ms 
Running dlrm: 6.66 ms, backpropagation after dlrm: 30.47 ms, backpass: 28.20 ms
Finished training it 16384/30697 of epoch 0, 37.42 ms/it, loss 0.464865
Embedding lookup time: 3.30 ms, MLP time: 1.90 ms, interaction time: 2.28 ms 
Running dlrm: 6.63 ms, backpropagation after dlrm: 30.40 ms, backpass: 28.12 ms
Finished training it 24576/30697 of epoch 0, 36.43 ms/it, loss 0.460121
Embedding lookup time: 3.30 ms, MLP time: 1.91 ms, interaction time: 2.28 ms 
Running dlrm: 6.63 ms, backpropagation after dlrm: 29.40 ms, backpass: 27.14 ms
Finished training it 30697/30697 of epoch 0, 37.24 ms/it, loss 0.457142
Embedding lookup time: 3.29 ms, MLP time: 1.90 ms, interaction time: 2.28 ms 
Running dlrm: 6.62 ms, backpropagation after dlrm: 30.22 ms, backpass: 27.97 ms
The MLP time is 58.4664671421051
The embedding time is 101.3839271068573
The interaction time is 70.05338668823242
The total time is 1372.7101576328278
Command used to run the program: dlrm_s_pytorch.py --thread-count=25 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 32
Finished training it 8192/30697 of epoch 0, 96.56 ms/it, loss 0.485913
Embedding lookup time: 43.11 ms, MLP time: 1.80 ms, interaction time: 4.98 ms 
Running dlrm: 47.86 ms, backpropagation after dlrm: 48.25 ms, backpass: 45.73 ms
Finished training it 16384/30697 of epoch 0, 98.91 ms/it, loss 0.464865
Embedding lookup time: 43.33 ms, MLP time: 1.85 ms, interaction time: 4.36 ms 
Running dlrm: 47.83 ms, backpropagation after dlrm: 50.61 ms, backpass: 47.63 ms
Finished training it 24576/30697 of epoch 0, 97.55 ms/it, loss 0.460121
Embedding lookup time: 43.37 ms, MLP time: 1.79 ms, interaction time: 4.28 ms 
Running dlrm: 47.78 ms, backpropagation after dlrm: 49.31 ms, backpass: 46.72 ms
Finished training it 30697/30697 of epoch 0, 97.49 ms/it, loss 0.457142
Embedding lookup time: 43.32 ms, MLP time: 1.85 ms, interaction time: 4.31 ms 
Running dlrm: 47.80 ms, backpropagation after dlrm: 49.22 ms, backpass: 46.65 ms
The MLP time is 55.92873764038086
The embedding time is 1328.4681091308594
The interaction time is 137.9803009033203
The total time is 3230.747001647949
Command used to run the program: dlrm_para_pytorch.py --thread-count=32 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 32
Finished training it 8192/30697 of epoch 0, 52.27 ms/it, loss 0.485913
Embedding lookup time: 3.41 ms, MLP time: 1.75 ms, interaction time: 2.33 ms 
Running dlrm: 6.61 ms, backpropagation after dlrm: 45.26 ms, backpass: 42.86 ms
Finished training it 16384/30697 of epoch 0, 49.40 ms/it, loss 0.464865
Embedding lookup time: 3.39 ms, MLP time: 1.74 ms, interaction time: 2.33 ms 
Running dlrm: 6.58 ms, backpropagation after dlrm: 42.42 ms, backpass: 40.04 ms
Finished training it 24576/30697 of epoch 0, 51.48 ms/it, loss 0.460121
Embedding lookup time: 3.40 ms, MLP time: 1.74 ms, interaction time: 2.33 ms 
Running dlrm: 6.60 ms, backpropagation after dlrm: 44.49 ms, backpass: 42.09 ms
Finished training it 30697/30697 of epoch 0, 50.85 ms/it, loss 0.457142
Embedding lookup time: 3.40 ms, MLP time: 1.74 ms, interaction time: 2.34 ms 
Running dlrm: 6.60 ms, backpropagation after dlrm: 43.85 ms, backpass: 41.45 ms
The MLP time is 53.42529344558716
The embedding time is 104.34718418121338
The interaction time is 71.65271520614624
The total time is 1800.2449917793274
Command used to run the program: dlrm_s_pytorch.py --thread-count=32 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
===============================================
===============================================

=== RUNNING dlrm_para_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 40
Finished training it 8192/30697 of epoch 0, 133.77 ms/it, loss 0.485913
Embedding lookup time: 44.06 ms, MLP time: 2.85 ms, interaction time: 9.99 ms 
Running dlrm: 52.38 ms, backpropagation after dlrm: 80.93 ms, backpass: 75.90 ms
Finished training it 16384/30697 of epoch 0, 132.06 ms/it, loss 0.464865
Embedding lookup time: 44.02 ms, MLP time: 3.12 ms, interaction time: 9.69 ms 
Running dlrm: 52.46 ms, backpropagation after dlrm: 79.14 ms, backpass: 73.55 ms
Finished training it 24576/30697 of epoch 0, 131.41 ms/it, loss 0.460121
Embedding lookup time: 44.29 ms, MLP time: 2.86 ms, interaction time: 9.03 ms 
Running dlrm: 52.14 ms, backpropagation after dlrm: 78.80 ms, backpass: 73.31 ms
Finished training it 30697/30697 of epoch 0, 133.13 ms/it, loss 0.457142
Embedding lookup time: 43.91 ms, MLP time: 2.99 ms, interaction time: 9.15 ms 
Running dlrm: 51.96 ms, backpropagation after dlrm: 80.71 ms, backpass: 75.65 ms
The MLP time is 90.57577610015869
The embedding time is 1353.1659588813782
The interaction time is 291.2198348045349
The total time is 4337.052012205124
Command used to run the program: dlrm_para_pytorch.py --thread-count=40 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16

=== RUNNING dlrm_s_pytorch.py ===
/users/mt1370/expr/dlrm_minrui/dlrm_data_pytorch.py:328: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)
world size: 1, current rank: 0, local rank: 0
Using CPU...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=./input/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
time/loss/accuracy (if enabled):
PyTorch Intra-op threads: 40
Finished training it 8192/30697 of epoch 0, 73.96 ms/it, loss 0.485913
Embedding lookup time: 3.61 ms, MLP time: 1.85 ms, interaction time: 2.35 ms 
Running dlrm: 6.92 ms, backpropagation after dlrm: 66.64 ms, backpass: 64.02 ms
Finished training it 16384/30697 of epoch 0, 70.87 ms/it, loss 0.464865
Embedding lookup time: 3.59 ms, MLP time: 1.85 ms, interaction time: 2.36 ms 
Running dlrm: 6.92 ms, backpropagation after dlrm: 63.56 ms, backpass: 60.91 ms
Finished training it 24576/30697 of epoch 0, 69.35 ms/it, loss 0.460121
Embedding lookup time: 3.58 ms, MLP time: 1.83 ms, interaction time: 2.35 ms 
Running dlrm: 6.87 ms, backpropagation after dlrm: 62.08 ms, backpass: 59.58 ms
Finished training it 30697/30697 of epoch 0, 70.81 ms/it, loss 0.457142
Embedding lookup time: 3.60 ms, MLP time: 1.82 ms, interaction time: 2.34 ms 
Running dlrm: 6.88 ms, backpropagation after dlrm: 63.53 ms, backpass: 61.01 ms
The MLP time is 56.47759222984314
The embedding time is 110.32192778587341
The interaction time is 72.17521977424622
The total time is 2426.3661444187164
Command used to run the program: dlrm_s_pytorch.py --thread-count=40 --arch-sparse-feature-size=16 --arch-mlp-bot=13-512-256-64-16 --arch-mlp-top=512-256-1 --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --dataset-multiprocessing --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=8192 --print-time --test-mini-batch-size=16384 --test-num-workers=16
